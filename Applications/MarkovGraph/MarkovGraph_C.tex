\lab{Applications}{Markov Chains I}{Markov Chains I}
\label{lab:MarkovGraph}

\objective{This section teaches about two simple applications of Linear Algebra.
First, it teaches about Markov Chains, which, in this context, represent discrete random transitions.
Second, it teaches about Graph Theory, which can be used to represent many physical problems.}

\section*{Markov Chains}

%Lab \ref{Markov}

A Markov Chain describes a particular type of random process characterized by the fact that all relevant information is related to its current state.
We can easily model this process using matrices.
We will start with a canonical example of a frog jumping from one lily pad to another.

Fredo the Frog hops around between the three lily pads 1, 2, and 3.
If he's on lily pad 1 and jumps, there is a 25\% chance that he will land back on lily pad 1, a 25\% chance that he will land on lily pad 2, and a 50\% chance that he will land on lily pad 3.
We can find similar probabilities if he starts on lily pad 2 or 3.
In figure \ref{markov1} we have a transition diagram that reflects the various probabilities that Fredo will go from one lily pad to another.

\begin{figure}[h!]
\begin{center}
\includegraphics[scale = 1.4]{markov1}
\end{center}
\caption{Transition diagram for Fredo the Frog}
\label{markov1}
\end{figure}

We can convert our transition diagram into a transition matrix, where the $(i,j)$-entry of the matrix corresponds to the probability that Fredo jumps from the $j^{th}$ lily pad to the $i^{th}$ lily pad (where $1$ is the first lily pad, $2$ is the second, and so on).
In Fredo's case, the transition matrix is
\[
A = \begin{pmatrix}
1/4 & 1/2 & 1/2\\
1/4 & 1/6 & 1/2\\
1/2 & 1/3 & 0
\end{pmatrix}
\]
Note that all of the columns add up to one.
This is important. %is it standard to have transitions be column-wise?

If Fredo is on lily pad 1, where will he be after two jumps?
By multiplying the matrix $A$ by itself, we have (approximately)

\[
A^2 = \begin{pmatrix}
0.4375 & 0.3750 & 0.3750\\
0.3542 & 0.3194 & 0.2083\\
0.2083 & 0.3056 & 0.4167
\end{pmatrix}
\]
From this, we infer that there is a 43.75\% chance he will still be on lily pad 1 after two jumps.
Note that he might have jumped from 1 to 1 to 1, denoted $1 \rightarrow 1 \rightarrow 1$, or he could have jumped to one of the other lily pads and then back again, that is, either $1 \rightarrow 2 \rightarrow 1$ or $1 \rightarrow 3 \rightarrow 1$.
In addition, there is a 35.42\% chance he will be on lily pad 2 and a 20.83\% chance that he will be on lily pad 3.
Using Python, we can type in our transition matrix and see where Fredo will be after 5, 10, 20 or 100 jumps.

\begin{lstlisting}
#Remember, the 1.'s in the numerator force floating point division
>>> A = np.array([[1./4,1./2,1./2],[1./4,1./6,1./2],[1./2,1./3,0]])
>>> np.linalg.matrix_power(A,5)
>>> np.linalg.matrix_power(A,10)
>>> np.linalg.matrix_power(A,20)
>>> np.linalg.matrix_power(A,100)
\end{lstlisting}

Note that as we take higher powers it appears that the limit goes to
\[
A^\infty = \begin{pmatrix}
0.4 & 0.4 & 0.4\\
0.3 & 0.3 & 0.3\\
0.3 & 0.3 & 0.3
\end{pmatrix},
\]
and this can, in fact, be proven carefully.
This means that after several jumps, the probability that we will find Fredo on a given lily pad will have nothing to do with where he started initially.

\section*{Markov Chains}

We can generalize this notion beyond that of frogs and lily pads.
Let the state of our system be represented by a probability vector
\[
\x = \begin{bmatrix}
x_1\\
x_2\\
\vdots\\
x_n
\end{bmatrix}
\]
where each entry represents the probability of being in that state.
Note that each entry is nonnegative and the sum of all the entries adds up to one.
For example, in the case of Fredo, if we know initially that he is on lily pad 1, then we have the state vector
\[
\x_0 = \begin{bmatrix}
1\\
0\\
0
\end{bmatrix}
\]
because we know for certainty (100\%) that Fredo is in the first state.
After one jump, we have
\[
\x_1 = A \x_0 = \begin{bmatrix}
0.25\\
0.25\\
0.50
\end{bmatrix}
\]
After two jumps, we have
\[
\x_2 = A \x_1 = A^2 \x_0 = \begin{bmatrix}
0.4375\\
0.3542\\
0.2083
\end{bmatrix}
\]
After a large number of jumps $(n>>1)$, we have
\[
\x_n = A \x_{n-1} = \dots = A^n \x_0 \approx \begin{bmatrix}
0.4\\
0.3\\
0.3
\end{bmatrix}
\]
Since all of the columns are the same for $A^\infty$, then for any initial probability vector $\x_0$, we get the same limiting output, or in other words, all initial vectors converge to the same point, call it $\x_\infty$.
Moreover, we have that
\[
\x_\infty = A \x_\infty
\]
This is called a stable fixed point.
How can we check that a stable fixed point exists?
(Hint: think eigenvalues and eigenvectors.)

\section*{Example}

Consider the Markov chain given by
\[
A = \begin{pmatrix}
0.5 & 0.3 & 0.4\\
0.2 & 0.2 & 0.3\\
0.3 & 0.5 & 0.3
\end{pmatrix}.
\]
We show that it has a stable fixed point by checking that it has a single eigenvalue $\lambda=1$.
We do this via Python:
\begin{lstlisting}
>>> A = np.array([[.5,.3,.4],[.2,.2,.3],[.3,.5,.3]])
>>> V = la.eig(A)[1]
\end{lstlisting}
Note that the entries in the $\lambda=1$ eigenvector do not generally add up to one.
Indeed, any multiple of an eigenvector is an eigenvector.
So we need to multiply it by the appropriate constant so that all of the entries add up to one.
\begin{lstlisting}
>>> x = V[:,0]
>>> x = x/sp.sum(x);x
array([ 0.41836735,  0.23469388,  0.34693878])
\end{lstlisting}
We can check this answer by taking $A$ to a high exponent, say $A^{100}$.

\begin{problem}
Suppose a basketball player's success at shooting free throws can be described with the following Markov chain
\[
A = \begin{pmatrix}.75&.50\\.25&.50\end{pmatrix}
\]
where the first state corresponds to success and the second state to failure.
\begin{enumerate}
\item If the player makes his first free throw, what is the probability that he also makes his third one?
\item What is the player's average free throw percentage?
\end{enumerate}
\end{problem}

\begin{problem}
Consider the Markov process given by the transition diagram in Figure \ref{markov2}.
\begin{enumerate}
\item Find the transition matrix.
\item If the Markov process is in state 1 initially, find the probability that it is in state 2 after 2 transitions.
\item Find the stable fixed point if it exists.
\end{enumerate}
\end{problem}

\begin{figure}[h!]
\begin{center}
\includegraphics[scale = 1.4]{markov2}
\end{center}
\caption{Transition diagram}
\label{markov2}
\end{figure}

\newpage

\section*{Graph Theory}
\begin{figure}[h!]
\includegraphics[scale = .4]{graphExample}
\caption{A simple graph}
\label{markov:example_graph}
\end{figure}

Graph theory is an important branch of mathematics and computer science.
It describes how objects are connected to one another.
Considered rigorously, a graph is composed of two sets: a set of nodes and a set of edges that describe the connections between the nodes.

A graph is \emph{directed} if connections are uni-directional, and \emph{undirected} if they are bi-directional.
Figure \ref{markov:example_graph} shows a simple undirected graph.
We can write a matrix that describes this type of graph.
We let each row of our matrix represent our starting point and each column represent our destination.
We put a 1 if there is a path and a 0 if there is not.
For the above graph we generate the following matrix:

\[
A = \begin{pmatrix}
0 & 1 & 0 & 0 & 1 & 0\\
1 & 0 & 1 & 0 & 1 & 0\\
0 & 1 & 0 & 1 & 0 & 0\\
0 & 0 & 1 & 0 & 1 & 1\\
1 & 1 & 0 & 1 & 0 & 0\\
0 & 0 & 0 & 1 & 0 & 0
\end{pmatrix}
\]

This matrix is called an \emph{adjacency matrix}, and in this case is 
symmetric since the graph is undirected.

What happens if we square an adjacency matrix?
It turns out that raising an adjacency matrix to the $n$ power yields the number of paths of length $n$ between two vertices.
For example, by squaring the above matrix Python gives:
\begin{lstlisting}
>>> np.linalg.matrix_power(A,2)
array([[2, 1, 1, 1, 1, 0],
       [1, 3, 0, 2, 1, 0],
       [1, 0, 2, 0, 2, 1],
       [1, 2, 0, 3, 0, 0],
       [1, 1, 2, 0, 3, 1],
       [0, 0, 1, 0, 1, 1]])
\end{lstlisting}

Now try to find the number of connections of length 6 from node 3 to itself.
This is simple to do in Python:
\begin{lstlisting}
>>> np.linalg.matrix_power(A,6)
array([[45, 54, 38, 45, 54, 16],
       [54, 86, 29, 77, 51, 11],
       [38, 29, 55, 15, 70, 27],
       [45, 77, 15, 75, 31,  4],
       [54, 51, 70, 31, 93, 34],
       [16, 11, 27,  4, 34, 14]])
\end{lstlisting}
We see that there are 55 unique paths of length 6 from node 3 to itself.
Imagine trying to count all of those paths by hand!
It would be very easy to count incorrectly.
However, this method makes it very simple to count paths without any mistakes.

Adjacency matrices can also be composed of \li{True} and \li{False} values. 
In this case, the $n^{th}$ power of such a matrix (using boolean arithmetic) 
is again a matrix of
boolean values which simply indicate whether there exists a path of length $n$ between the given pair of nodes, rather than indicating the number of such
paths.

\begin{problem}
Let the following matrix represent a directed graph
\begin{lstlisting}
>>> A = 
np.array([[0, 0, 1, 0, 1, 0, 1],
          [1, 0, 0, 0, 0, 1, 0],
          [0, 0, 0, 0, 0, 1, 0],
          [1, 0, 0, 0, 1, 0, 0],
          [0, 0, 0, 1, 0, 0, 0],
          [0, 0, 1, 0, 0, 0, 1],
          [0, 1, 0, 0, 0, 0, 0]])
\end{lstlisting}

Between which pair of nodes does there exist the greatest number of paths
of length five?
From which node to which node is there no path of length seven?
\end{problem}

It turns out that the study of graphs and connectivity has a variety of applications.
For example, connections between web pages can be described as graphs.
So can flights between airports or friends on social networking sites.
The same ideas are applied frequently in computer chip design and in the preservation of endangered species.

We can use an adjacency matrix to map out paths from one part of our graph to another.
Consider the maze in Figure \ref{maze_fig}. We can interpret the maze as a 
graph by considering the 225 sites as the set of nodes, and two sites have 
and edge connecting them if they are adjacent and not separated by a maze 
wall (shown by the lines in the maze figure). Thus, although node 0 is 
adjacent to nodes 15 and 1 in the maze figure, it is only connected to node 
15. 

\begin{figure}
\includegraphics[width=.8\textwidth]{maze.pdf}
\caption{A maze for problem \ref{maze_prob}.}
\label{maze_fig}
\end{figure}

With this lab we have included the file \li{maze.npy} containing the adjacency matrix of this graph. Recall that you can load this matrix using the command 
\li{np.load('maze.npy')}.

\begin{problem} \label{maze_prob}
Write a function that takes as arguments a start node (integer), end
node (integer), and boolean adjacency matrix for a graph. The function should
return a list of integers representing a shortest path from the start node
to the end node (there may be more than one, just return one).
Use the function you just wrote to solve the maze shown in figure \ref{maze_fig}.

Here is how this can be done:
\begin{itemize}
\item Make a list containing the adjacency matrix $A$, set a count variable \li{n = 1}.
\item While the start node is not connected to the end node in $A^n$, 
    append $A^{n+1}$ to the list and increment $n$. If $n$ exceeds the 
    number of nodes, raise and error, since no path between the start 
    and end nodes exists. 
\item Now construct a path from the start node to the end node, as follows.
    Let $L$ be the length of the shortest path from the start node 
    to the end node (this is final value of the count variable $n$), 
    and suppose we know the $i^{th}$ node along the path. To find the 
    $(i+1)^{th}$ node, simply find a node that is both connected to node 
    $i$ in $A$ and connected to the end node in $A^{L-i}$. 
\end{itemize}

This approach is by no means optimal, but it illustrates some of the 
usefulness of adjacency matrices in extracting information about a graph.
\end{problem}


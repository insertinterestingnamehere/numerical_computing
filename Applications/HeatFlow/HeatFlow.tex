\lab{Algorithms}{Conservation laws and heat flow}{Conservation laws and heat flow}
\label{lab:finitedifference1}
A conservation law is a balance law, and corresponds to an equation that describes how a quantity is balanced in some system throughout a given process.
(Consider how this is related to conservation laws in physics.)
For example, suppose we are keeping track of some measurable quantity in a physical system.
The fundamental conservation law can be expressed in the following way: the rate of change of the total amount of the quantity in the system equals the rate of the quantity flowing into the system plus the rate that the quantity is produced by sources inside the system. 

\section{Derivation of the Conservation equation in multiple dimensions}
Suppose $\Omega$ is a region in $\mathbb{R}^n$, and $V \subset \Omega$ is bounded with a nice, well-behaved boundary $\partial V$.
Let $u(\vec{x},t)$ represent the density (concentration) of some quantity throughout $\Omega$.
Let $\vec{n}(x)$ represent the normal direction to $V$ at $x \in \partial V$, and let $\vec{J}(\vec{x},t)$ be the flux vector for the quantity, so that $\vec{J}(\vec{x},t) \cdot \vec{n}(x) \, dA$ represents the rate at which the quantity leaves $V$ by crossing a boundary element with area $dA$.
Note that the total amount of the quantity in $V$ is 
\[ \int_V u(\vec{x},t)\, dt,\]
and the rate at which the quantity enters $V$ is 
\[-\int_{\partial V} \vec{J}(\vec{x},t) \cdot \vec{n}(x) \, dA.\]

We let the source term be given by $f(\vec{x},t,u)$; we may interpret this to mean that the rate at which the quantity is produced in $V$ is 
\[ \int_V f(\vec{x},t,u)\, dt.\]
Then the integral form of the conservation law for $u$ is expressed as 
\[\frac{d}{dt} \int_V u(\vec{x},t) \, d\vec{x} = -\int_V \vec{J}\cdot \vec{n}\, dA + \int_V f(\vec{x},t,u)\, d\vec{x}.\]

If $u$ and $J$ are sufficiently smooth functions, then we have 
\[ \frac{d}{dt} \int_V u\, d\vec{x} = \int_V u_t \, d\vec{x},\]
and 
\[ \int_V \vec{J}\cdot \vec{n}\, dA = \int_V \nabla \cdot \vec{J}\, d\vec{x} .\]
Since this holds for all nice subsets $V \subset \Omega$ with $V$ arbitrarily small, we obtain the differential form of the conservation law for $u$: 
\[ u_t + \nabla \cdot \vec{J} = f(\vec{x},t,u) .\]

\section{Constitutive Relations}
Currently our conservation law appears in the form 
\[u_t + \nabla \vec{J} = f(\vec{x},t,u).\]
Thus the conservation law consists of one equation and 2 unknowns ($u$ and $J$).
To this equation we add other equations, called constituve relations, which are used to fully determine the system. 

For example, suppose we wish to describe the flow of heat.
Since heat flows from warmer regions to colder regions, and the rate of heat flow depends on the difference in temperature between regions, we usually assume that the flux vector $\vec{J}$ is given by 
\[
\vec{J}(x,t) = -\nu \nabla u(x,t),
\]
where $\nu$ is the diffusion constant.
This constitutive relation is called Fick's law, and is the basic model for any diffusive process.
Substituting into the conservation law we obtain 
\[u_t -\nu \triangle u(x,t) = f(\vec{x},t,u).\]  
The function $f$ represents heat sources/sinks within the region. 

\section{Numerically modelling heat flow}
Consider the heat flow equation in one dimension together with appropriate initial conditions and homogeneous Dirichlet boundary conditions: 
\begin{align*}
	&{ } u_t = \nu u_{xx}, \quad x \in [a,b],\quad t \in [0,T], \\
	&{ } u(a,t) = 0,\quad u(b,t) = 0,\\
	&{ } u(x,0) = f(x).
\end{align*}
We will look for an approximation $U^j_i$ to $u(x_i,t_j)$ on the grid $x_i = a +  hi$, $t_j = kj$, where $h = \triangle x$, $k = \triangle t$.

A common method for modelling ordinary and partial differential equations is the finite difference method, so-named because equations containing derivatives are replaced with equations containing difference schemes.
These difference schemes can often be found using Taylor's theorem.
For example, the equation 
\begin{align*}
	u(x,t_j + k) = u(x,t_j) + u_t(x,t_j)k + \mathcal{O}(k^2)
\end{align*}
yields a first-order forward difference approximation to $u_t(x,t_j)$, namely, 
\begin{align*}
	u_t(x,t_j ) = \frac{u(x,t_j+k) - u(x,t_j)}{k} + \mathcal{O}(k).
\end{align*}
Similarly, by adding the equations 
\begin{align*}
	u(x_i+h,t) &= u(x_i,t) + u_x(x_i,t)h + u_{xx}(x_i,t)\frac{h^2}{2} + u_{xxx}(x_i,t)h^3 + \mathcal{O}(h^4),\\
	u(x_i-h,t) &= u(x_i,t) + u_x(x_i,t)(-h) + u_{xx}(x_i,t)\frac{(-h)^2}{2} + u_{xxx}(x_i,t)(-h)^3 + \mathcal{O}(h^4),
\end{align*}
we obtain a second-order centered difference approximation to $u_{xx}(x_i,t)$:
\begin{align*}
	u_{xx}(x_i,t_j) &= \frac{u(x_i + h,t_j )-2 u(x_i,t_j)- u(x_i - h,t_j)}{h^2} + \mathcal{O}(h^2).
\end{align*}


% A simple explicit method for numerically approximating the solution of this problem is to use a forward difference approximation in time and a centered difference approximation in space.
% Recall that
% \[u_t(x_i,t_j) = \frac{u(x_i,t_j + k) - u(x_i,t_j)}{k} + \mathcal{O}(k)
% \]
%  and 
% \[u_{xx}(x_i,t_j) = \frac{u(x_i + h,t_j )-2 u(x_i,t_j)- u(x_i - h,t_j)}{h^2} + \mathcal{O}(h^2).
% \]
% Let $U_{i,j}$ represent our approximation to $u(x_i,t_j)$. 
These difference approximations give us the $\mathcal{O}(h^2 + k)$ explicit method 
\begin{align}
	\begin{split}
	\frac{U_{i}^{j+1} - U_{i}^{j}}{k} &= \nu \frac{U_{i+1}^{j}- 2U_{i}^{j} + U_{i-1}^{j} }{h^2} ,\\ 
	U_{i}^{j+1} &= U_{i}^{j} + \frac{\nu k}{h^2} (U_{i+1}^{j}- 2U_{i}^{j} + U_{i-1}^{j} ). 
	\end{split}\label{firstorder_explicit}
\end{align}
This method can be written in matrix form as 
\[
U^{j+1} = A U^j,
\]
where $A$ is the tridiagonal matrix given by 
\[
A = \left[\begin{array}{cccccc}1-2\lambda & \lambda &  &  &   \\ \lambda & 1-2\lambda & \lambda &  &   \\  & \ddots & \ddots & \ddots &  \\  &  & \lambda & 1-2\lambda & \lambda \\  &  &  & \lambda & 1-2\lambda\end{array}\right],
\]
$\lambda = \nu k/h^2$, and $U^j$ represents the approximation at time $t_j$.
We can get this method started by using the initial condition given in our problem, so that $U_{i}^{0} = f(x_i)$. 

\begin{figure}[ht]
\centering
\includegraphics[width=9cm]{heatexercise1a.pdf}
\caption{The graph of $U^{0}$, the approximation to the solution $u(x,t=0)$ for \ref{heat_exercise1}.}
\label{figure1a}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=9cm]{heatexercise1b.pdf}
\caption{The graph of $U^{10}$, the approximation to the solution $u(x,t=.4)$ for \ref{heat_exercise1}.}
\label{figure1b}
\end{figure}


\begin{problem}
Consider the specific initial boundary value problem
\begin{align}
	\begin{split}
	&{ } u_t = .05 u_{xx}, \quad x \in [0,1], \\
	&{ } u(0,t) = 0,\quad u(1,t) = 0,\\
	&{ } u(x,0) = 2\max\{.2 - |x-5.|,0\}.
	\end{split}\label{heat_exercise1}
\end{align}
Approximate the solution $u(x,t)$ at time $t = .4$ be taking 6 subintervals in the $x$ dimension and 10 subintervals in time. 
The graphs for $U^0$ and $U^{10}$ are given in Figures \eqref{figure1a} and \ref{figure1b}.
\end{problem}


\begin{problem}
Solve the specific initial boundary value problem
\begin{align}
	\begin{split}
	&{ } u_t = u_{xx}, \quad x \in [-12,12],\quad t \in [0,1], \\
	&{ } u(0,t) = 0,\quad u(1,t) = 0,\\
	&{ } u(x,0) = \max\{1 - x^2,0\}
	\end{split}\label{heat_exercise2}
\end{align}
using the first order explicit method \ref{firstorder_explicit}.
Use 140 subintervals in the $x$ dimension and 70 subintervals in time.
The initial and final states are shown in \ref{heat_exercise2}.
Animate your results. 

Explicit methods usually have a stability condition, called a CFL condition (for Courant-Friedrichs-Lewy). For method \ref{firstorder_explicit} the CFL condition that must be satisfied is that 
\[ \frac{\nu \triangle t}{(\triangle x)^2} \leq \frac{1}{2}.\]
Repeat your computations using 140 subintervals in the $x$ dimension and 66 subintervals in time.
For these values the CFL condition is broken; you should easily see the result of this instability in the approximation $U^{66}$.
% Then the graphs for $U^0$ and $U^{70}$ are given in \eqref{heatexercise2a} and \ref{heatexercise2b}.
\end{problem}

\begin{figure}[ht]
\centering
\includegraphics[width=12cm]{heatexercise2.pdf}
\caption{The initial and final states for \ref{heat_exercise2}.}
\label{figure2}
\end{figure}

Implicit methods often have better stability properties than explicit methods.
The Crank-Nicolson method for example is unconditionally stable and has order $\mathcal{O}(h^2 + k^2)$.
To derive the Crank-Nicolson method we use the following approximations: 
\begin{align*}
	u_t(x_i,t_{j+1/2}) &= \frac{u_t(x_i,t_{j+1}) - u_t(x_i,t_j)}{k} + \mathcal{O}(k^2), \\
	u_{xx}(x_i,t_{j+1/2}) &= \frac{u_{xx}(x_i,t_{j+1}) + u_{xx}(x_i,t_j)}{2} + \mathcal{O}(k^2).
\end{align*}
These approximations give the method 
\begin{align}
	\begin{split}
	\frac{U^{j+1}_i - U^j_i}{k} &= \frac{1}{2}\left( \frac{U^j_{i+1} - 2U^j_{i} + U^j_{i-1}}{h^2} + \frac{U^{j+1}_{i+1} - 2U^{j+1}_{i} + U^{j+1}_{i-1}}{h^2}  \right) ,\\
	U^{j+1}_i  &= U^j_i + \frac{k}{2h^2} \left( U^j_{i+1} - 2U^j_{i} + U^j_{i-1} + U^{j+1}_{i+1} - 2U^{j+1}_{i} + U^{j+1}_{i-1}   \right).
\end{split}
\end{align}
This method can be written in matrix form as 
\[BU^{j+1} = A U^j,\]
where $A$ and $B$ are tridiagonal matrices given by 
\begin{align*}
B &= \left[\begin{array}{cccccc}1+2\lambda & -\lambda &  &  &  \\ -\lambda & 1+2\lambda &  -\lambda & &  \\ &  \ddots &   \ddots & \ddots \\ & &  -\lambda &  1+2\lambda & -\lambda \\ &  &  & -\lambda & 1+2\lambda\end{array}\right], \\
A &= \left[\begin{array}{cccccc}1-2\lambda & \lambda &  &  &  \\ \lambda & 1-2\lambda &  \lambda & &  \\ &  \ddots &   \ddots & \ddots \\ & &  \lambda &  1-2\lambda & \lambda \\ &  &  & \lambda & 1-2\lambda\end{array}\right], 
\end{align*}
where $\lambda = \nu k/(2h^2)$, and $U^j$ represents the approximation at time $t_j$.

How do we know if a numerical approximation is reasonable?
One way to determine this is to compute solutions for various step sizes $h$ and see if the solutions are converging to something.
To be more specific, suppose our finite difference method is $\mathcal{O}(h^p)$ accurate.
This means that the error $E(h) \approx Ch^p$ for some constant $C$ as $h \to 0$ (i.e., for $h>0$ small enough).

So compute the approximation $y_k$ for each stepsize $h_k$, $h_1 > h_2> \ldots>h_m.$  We will think of $y_m$ as the true solution.
Then the error of the approximation for stepsize $h_k, k < m,$ is 
\begin{align*}
	E(h_k) &= \max( \abs{ y_k - y_m}) \approx C h_k^p ,\\
	\log(E(h_k)) &= \log(C) + p \log(h_k).
\end{align*}
Thus on a log-log plot of $E(h)$ vs. $h,$ these values should be on a straight line with slope $p$ when $h$ is small enough to start getting convergence.

\begin{figure}[ht]
\centering
\includegraphics[width=12cm]{MaximumError.pdf}
\caption{$E(h)$ represents the (approximate) maximum error in the numerical solution $U$ to problem \eqref{heat_exercise3} at time $t=1$, using a stepsize of $h$.}
\label{figure3}
\end{figure}



\begin{problem}
Using the Crank Nicolson method, numerically approximate the solution $u(x,t)$ of the problem 
\begin{align}
	\begin{split}
	&{ } u_t = u_{xx}, \quad x \in [-12,12],\quad t \in [0,1], \\
	&{ } u(0,t) = 0,\quad u(1,t) = 0,\\
	&{ } u(x,0) = \max\{1 - x^2,0\}.
	\end{split}\label{heat_exercise3}
\end{align}
Demonstrate that the numerical approximation at $t = 1$ converges to  $u(x,t=1)$.
(Hint: Compute $U$ at $t=1$ using stepsizes $h=k=20,40,80,160,320$, and $640$.
Reproduce Figure \eqref{figure3}.
Notice that since the Crank-Nicolson method is unconditionally stable, there is no CFL condition and we can let $h=k$.)
\end{problem}

% The matrix $A$ is sparse, and so we can use several functions from the package \texttt{scipy.sparse.linalg}.
% In particular, we use the functions \texttt{spdiags} and \texttt{spsolve}.
% 
% \begin{verbatim}
% D1,D2,D3 = -4*np.ones((1,m**2)), np.ones((1,m**2)), np.ones((1,m**2)) 
% Dm = np.ones((1,m**2))
% for j in range(0,D2.shape[1]):
%     if (j%m)==m-1:
%         D2[0,j]=0
%         if (j%m)==0:
%             D3[0,j]=a0
% diags = np.array([0,-1,1,-m,m])
% data = np.concatenate((D1,D2,D3,Dm,Dm),axis=0)
% 
% A = 1./h**2.*spdiags(data, diags, m**2,m**2).asformat('csr') 
% \end{verbatim}
% 
% \subsection{2D Heat Equation}
% Recall that the collection of finite difference equations
% \[
% \nabla^2_h U_{ij} = 0, \quad 1 \leq i,j\leq m,
% \]
% can be written in matrix form as
% $$AU + q  = 0$$
% 
% 
% The Crank-Nicolson method for the 2D heat equation is given by 
% \[U_{i,\,j}^{n+1}- U_{i,\,j}^{n} = \frac{\Delta t}{2}(\nabla_h^2 U_{i,\,j}^{n} + \nabla_h^2 U_{i,\,j}^{n+1}) \text{ for each } 1 \leq i,j \leq m, \]
% is a second order accurate in both space and time. Basically we're using a midpoint scheme in time, 
% and a trapezoidal scheme in space. The resulting method is implicit, and can be written in matrix form as 
% \begin{align*}
% 	IU^{n+1} &= IU^n + \frac{\Delta t}{2}(AU^n + q + AU^{n+1} + q),\\
% 	(I - \frac{\Delta t}{2}A)U^{n+1}&= (I + \frac{\Delta t}{2}A)U^n + \Delta t q.	
% \end{align*}
% 
% TODO: What size must the time step be to ensure stability? 
% 
% We will need to take many time steps, where many equations must be solved with the matrix $(I - \frac{\Delta t}{2}A)$.
% The function \texttt{factorized} from \texttt{scipy.sparse.linalg} computes the LU decomposition of the matrix.
% This decomposition reduces the time required for solving consecutive time steps. 
% 
\lab{Algorithms}{Moore-Penrose Pseudo-Inverse}{Moore-Penrose}

\objective{This section explains several methods for numerically computing the Moore-Penrose Pseudo-Inverse. It also compares performance of the different methods.}
 
The generalized inverse of a matrix $A$ is a matrix (which we will denote $A^\dagger$ for now) that satisfies the following:
\begin{equation} \label{cond:one}
AA^\dagger A = A 
\end{equation}

\begin{equation} \label{cond:two}
A^\dagger A A^\dagger = A^\dagger 
\end{equation}

For any matrix $A$ there exists a generalized inverse. Note that in the case that $A$ is invertible, $A^{-1}$ satisfies the conditions \ref{cond:one} and \ref{cond:two}. However, it is not generally unique. For this reason, we usually add additional conditions that make the generalized inverse unique.

For example, we can specify the following two additional conditions:

\begin{equation} \label{cond:three}
(AA^\dagger)^* = AA^\dagger
\end{equation}

\begin{equation} \label{cond:four}
(A^\dagger A)^* = A^\dagger A
\end{equation}

These four conditions guarantee both the existence and uniqueness of the matrix $A^\dagger$ for any matrix $A$. The matrix $A^\dagger$ is known as the Moore-Penrose inverse. In some settings it is simply called the pseudo-inverse.\footnote{For the rest of this section we will use the notation $A^\dagger$ to represent the Moore-Penrose inverse, although in some books that notation is used a generalized inverse.}

%{\bf We could add some exposition here about examples of the MP inverse for specific matrices, but I don't know if Jeff is doing that in the book or not...}

\subsection{Calculating the Moore-Penrose Inverse}

There are several different methods for calculating the Moore-Penrose Inverse. We will consider the most general method first, and then compare its performance to a few specialized methods.

Recall that the SVD of a matrix $A$ is of the form

\[
A = U \Sigma V^*
\]

Where $U$ and $V$ are unitary (i.e. $U^*U = I$) and $\Sigma$ is diagonal. Consider the matrix

\[
B = V \Sigma^\dagger U^*
\]

Where $\Sigma^\dagger$ is the pseudo-inverse of $\Sigma$. Since $\Sigma$ is diagonal, the pseudo-inverse is simply found by replacing each non-zero entry with its multiplicative inverse and transposing the matrix. We write, for ease of notation $\Sigma^\dagger \Sigma = I_A$ Now we have the following:

\[
ABA = U \Sigma V^* = A
\]

\[
BAB = V \Sigma^\dagger U^* = B
\]

\[
(AB)^* = U \Sigma^{\dagger *} V^* V \Sigma^* U^* = U (\Sigma \Sigma^\dagger)^* U^* = U \Sigma \Sigma^\dagger U^* = U \Sigma V^* V \Sigma^\dagger U^* = AB
\]

\[
(BA)^* =  V \Sigma^* U^* U \Sigma^{\dagger *} V^* = V (\Sigma^\dagger \Sigma)^* V^* = V  \Sigma^\dagger \Sigma V^* =  V \Sigma^\dagger U^* U \Sigma V^* = BA
\]

Thus $B$ satisfies all of the properties of the pseudo-inverse, and since the pseudo-inverse is unique it is the only such matrix.

Therefore, we can easily calculate the pseudo-inverse of a matrix using the SVD. Write the following code in Python:

\begin{lstlisting}[style=python]
U,s,Vh = la.svd(A,full_matrices=False)
S = sp.diag(s)
\end{lstlisting}
We can then calculate the pseudo-inverse by using the following line of code:

\begin{lstlisting}[style=python]
Apinv = sp.dot(sp.dot(Vh.T,sp.diag(1./s)),U.T)
\end{lstlisting}

This line uses the matrices that \li{svd} created to calculate the pseudo-inverse, using the formula we established above. Note that we used \li{diag} twice to invert just the singular values (and not the zeros). 
\begin{problem}
Python has the built-in functions \li{la.pinv} and \li{la.pinv2} to calculate the pseudo-inverse. Write a script that compares the performance of \li{la.pinv},\li{la.pinv2}, and the implementation that we demonstrated above. Use a matrix generated by  \li{sp.rand}.
 Try the following matrix sizes:
\begin{itemize}
\item $1000 \times 1$
\item $1000 \times 20$
\item $1000 \times 500$
\item $1000 \times 1000$
\end{itemize}
How do the performances compare?
\end{problem}

\begin{problem}
{\bf Continuity of the pseudo-inverse:} This problem demonstrates why the pseudo-inverse is not generally a continous operation. Create a random $5 \times 5$ matrix $A$, and find its SVD using the code:
%should we use 'econ' here? I haven't read whatever section that comes from
\begin{lstlisting}[style=python]
U,s,Vh = la.svd(A,full_matrices=False)
V = Vh.T
S = sp.diag(s)
\end{lstlisting}
Set the bottom right entry of $S$ to zero, and then set $A = U*S*V'$. Now set $S(5,5)$ to be $.01$. Now calculate \li{la.norm(A - sp.dot(sp.dot(U,S),Vh))}. The value should be $.01$. This is because the matrix $2$-norm is simply the value of the largest singular value. Now calculate \li{la.norm(la.pinv(A) - la.pinv(sp.dot(sp.dot(U,S),Vh)))}. You should get $100$. Why is this? Hint: Think of how we calculated the pseudo-inverse. This example, and your explanation, should explain why two matrices, that are arbitrarily close together, can have pseudo-inverses that are arbitrarily far apart. Thus the pseudo-inverse is not a continuous operator.
\end{problem}

\subsection*{Other methods for calculating the Pseudo-Inverse}

The SVD is a powerful tool for calculating the pseudo-inverse. However, for large matrices it can be costly to calculate the SVD. Accordingly, there are several other methods for calculating the pseudo-inverse of a matrix.

One method is an iterative approach established by Ben-Israel and Cohen\footnote{Ben-Israel, Adi; Cohen, Dan (1966). "On Iterative Computation of Generalized Inverses and Associated Projections". SIAM Journal on Numerical Analysis 3: 410â€“419.}. This method uses the recursive sequence

\[
A_{i+1} = 2A_i - A_i A A_i
\]

With $A_0$ satisfying the equation $A_0 A = (A_0 A)^*$. The convergence of this sequence eventually becomes quadractic, which is a very desirable property. The simplest choice for $A_0$ is $\alpha A^*$, with $0 < \alpha < 2/\sigma_1^2(A)$, where $\sigma_1$ denotes the largest singular value (the condition on $\alpha$ is a technical condition established in the original paper to guarantee fast convergence).

One tricky part of developing this method is finding an appropriate $\alpha$. We don't want to waste much time calculating $\sigma_1$. Luckily there is an easy equivalence that we can leverage:

\[
||A||_2 = \sigma_1(A) = \sqrt{\lambda_{max} (A^* A)}
\]
%Python: no eigs function -- but it appears to be a topic of interest in the community, perhaps a future addition
Thus, we can calculate the largest singular value of $A$ quickly in some cases using. We stress that this is not always the fastest way. For example, if $A$ is a million by one column vector then calculating the 2-norm of $A$ is much faster than calculating the largest eigenvalue of a million by million matrix. However, with these equivalences we have some tools at our disposal to easily calculate the largest singular value of $A$.

Now, using these tools, we can write a simple function to calculate the pseudo-inverse using this iterative method:

\begin{lstlisting}[style=python]
function out = IterativePInv(A)

alpha = 2/(1.1*eigs(A'*A,1));
out = alpha*A';
C = out + 1;
while max(sum(abs(C-out))) > 1e-4
    C = out;
    out = 2*out - out*A*out;
end
\end{lstlisting}
As you can see, the implementation of this algorithm is fairly straightforward. We used the matrix $1$-norm (the maximum absolute column sum) to detect convergence of our sequence, since it is very fast to compute.

\begin{problem}
The implementation we wrote above is naive in a few ways. Make the following three adaptations to your code:
\begin{itemize}
\item Allow variable tolerance in the detection of convergence of the sequence. Use \li{*args} so that the user only specifies the tolerance if they want to.
\item We used the value $1.1$ rather arbitrarily in our selection of $\alpha$. The original paper proves that the optimal value is $2/(\sigma_1^2 + \sigma_r^2)$, where $\sigma_r$ is the smallest non-zero singular value. Use again to find the value of $\sigma_r$, and use that value to set $\alpha$.
\item If $A$ isn't close to square then it is not advantageous to use at all, as we explained above. Adapt the code to only use \li{eigs} as you think it's appropriate. Remember that you can use the matrix $2$-norm and a constant like $1.1$ in cases where  isn't appropriate.
\end{itemize}

Now test your code against the svd method, using the same cases that you did for that method. How does it perform?
\end{problem}

It should be noted that this method, although powerful, sometimes performs worse than the svd method. Specifically, when the matrix is ill-conditioned, it may take a long time for the sequence to enter the region of quadratic convergence, making this method a poor choice.

\subsection*{The QR method}

This final method, while only applicable in certain cases, offers a very fast method for calculating the pseudo-inverse. Suppose that $A$ has full column rank. We could show that the pseudo-inverse is then:
\[
A^\dagger = (A^* A)^{-1} A^*
\]

Direct calculation of the pseudo-inverse using this formula is not very helpful, since matrix inversion is so costly. However, since $A$ is full rank, then $A^*A$ is positive definite, and thus has a cholesky decomposition. Therefore, we can rewrite this equation as:

\[
R^*R A^\dagger = A^*
\]

with $R$ being upper-triangular. This equation can be solved quickly by using forward and backward substitution.

We can speed calculation even further by noting that we don't even have to calculate $A^* A$, which can be costly (again, imagine the million by one vector). We can calculate R using the QR-decomposition of A:

\[
A^* A = R^*Q^*QR = R^* R
\]

These equalities hold since $Q$ is orthogonal. Further, in the QR decomposition $R$ is upper-triangular, and thus we can calculate the cholesky factorization easily using the QR decomposition.

Combining these facts we can write the following code to calculate the pseudo-inverse for a matrix that has full column rank as follows:

\begin{lstlisting}[style=python]
R = la.qr(A)[1];

Ainv = la.lstsq(R,la.lstsq(R.T,A.T)[0])[0];
\end{lstlisting}


The first line performs QR decomposition for $R$. The second line conducts the backwards and forwards substitutions.

\begin{problem}
%This Problem did not exactly work in Python, my function was slower when A was square when using la.lstsq
Compare the run-time of this code against SciPy's \li{la.pinv}, for the values used in the other problems. Also, make sure to use a non-square matrix for A, or for square matricies change the \li{la.lstsq}'s to \li{la.solve}'s to maximize performance.  How does it compare? It should be a lot faster. This demonstrates that by leveraging the correct information (full column rank) we can significantly improve the speed of this operation. Note that we can beat SciPy's own implementation significantly in this case.

\end{problem}

\begin{problem}
This method can also be adapted to use for matrices that have full row rank. In the case of full column rank the following formula holds:
\[
A^\dagger = A^*(A A^*)^{-1}
\]
Use the same technique to write a function that finds the pseudo-inverse for a matrix of full row rank. Note that you will have to find the QR-decomposition of $A^*$ this time, and that it may be helpful to take the transpose so that you can use the backslash operator.
\end{problem}

\lab{Algorithms}{LU Decomposition}{LU Decomposition}
\label{lab:LUdecomp}
\objective{In this section we will find the REF and the LU decomposition.}

In linear algebra there are three elementary row operations: switching two rows, multiplying a row by a constant, and adding a multiple of one row to another row.
Each of these operations can, in theory, be done by left multiplication by a corresponding elementary matrix.
This approach is \emph{extremely} slow in practice.
In practice, it is much faster to perform these operations directly by modifying only the portions of an array that change as a result of the row operation.
The follow code shows how these modifications can be made in-place to an array.
\lstinputlisting[style=python,name=]{row_opers.py}

\section*{Programming Row Reduction}
Solving a linear system can be done most efficiently by using elementary row operations to reduce a matrix to \emph{row echelon form} (REF), as opposed to \emph{reduced row echelon form} (RREF).
Consider the following matrix: 
\[
\begin{pmatrix}
4&5&6&3 \\
2&4&6&4 \\
7&8&0&5
\end{pmatrix}
\]
Using elementary row operations, we can reduce $A$ to REF as follows:
\begin{lstlisting}
: import numpy as np
: A = np.array([[4., 5., 6., 3.],[2., 4., 6., 4.],[7., 8., 0., 5.]])
array([[ 4.,  5.,  6.,  3.],
       [ 2.,  4.,  6.,  4.],
       [ 7.,  8.,  0.,  5.]])
: A[1] -= (A[1,0]/A[0,0]) * A[0]
: A[2] -= (A[2,0]/A[0,0]) * A[0]
: A[2,1:] -= (A[2,1]/A[1,1]) * A[1,1:]
: A
array([[ 4. ,  5. ,  6. ,  3. ],
       [ 0. ,  1.5,  3. ,  2.5],
       [ 0. ,  0. , -9. ,  1. ]])
\end{lstlisting}
The additional requirement is often added that the first nonzero entry of each row be 1.
Do not worry about that requirement here.
Notice that in our third row operation we were able to operate on only a portion of the third row because we knew that the first value would still be 0.
In this case it made little difference, but it is good to watch for things like this because they can save a great deal more time when working with larger matrices.

\begin{problem}
\label{prob:REF}
Write a Python function, which takes a matrix and reduces it to REF.
Assume that the matrix is invertible and ignore the possibility that a zero may appear on the main diagonal during row reduction.
\end{problem}

\section*{LU Decomposition}
Using row reduction we can reduce an invertible matrix $A$ to upper triangular form.
Say this can be done in $k$ row operations.
Let $U$ be the upper triangular form of $A$, so we have:
Hence, we have
\[
U = E_k \dots E_2 E_1 A.
\]
Since the elementary matrices are invertible, we also have
\[
(E_k \dots E_2 E_1)^{-1} U =  A.
\]
Then we define $L$ to be
\[
L = (E_k \dots E_2 E_1)^{-1}
\]
which is the same as
\[
L = E_1^{-1} E_2^{-1} \dots E_k^{-1}
\]
In either case, we have $L U = A$.

The inverses of elementary matrices are also elementary matrices. $L$ can be computed by applying a series of simple operations to an identity matrix.
As it turns out, when we are only doing type 3 row operations, each of the operations represented by right multiplication by these inverse matrices results in a change in a single entry of $L$.

In practice, the LU decomposition of an array $A$ can be computed like this:
\begin{itemize}
\item Make a copy $U$ of $A$.
\item Make an identity matrix $L$ of the same shape as $A$.
\item Iterate through the entries below the diagonal of $U$.
For each entry below the main diagonal of $U$ do the following:
	\begin{itemize}
	\item Set the corresponding entry of $L$ to the quotient of the current entry of $U$ and the entry of the main diagonal of $U$ located above the current entry.
	\item Perform the type 3 row operation to set the current entry of $U$ to 0.
		Remember to avoid computation involving columns that have already been processed.
	\end{itemize}
\item Return $L$ and $U$
\end{itemize}
In this case, we have ignored the possibility that a 0 may appear along the main diagonal during computation.
A full implementation of the LU decomposition would have to account for this possibility as well.

\section*{Why This Matters}
The LU decomposition is more efficient for solving linear systems than traditional row reduction.
It also can be applied to matrices with more than one column.
The $LU$ decomposition also allows quick computation of inverses and determinants.
For very large matrices, the LU decomposition can be performed without using any extra space.
$L$ can be stored above the main diagonal of the array and $U$ can be stored below it.
There is no need to store the main diagonal of $L$ since all its entries are ones.

\begin{problem}
\label{prob:LU}
Write a Python function takes as input an $n\times n$ matrix, performs the LU decomposition and returns $L$ and $U$.
To verify that it works, multiply $L$ and $U$ together and compare to $A$.
Assume that the matrix is invertible and ignore the possibility that a zero may appear on the main diagonal during row reduction.

Write another version of the function that modifies its input in place, storing $L$ below the main diagonal and $U$ in the rest of the array.
\end{problem}

\begin{problem}
\label{prob:lusolve}
Write a function that takes the LU decomposition computed by the second function you made in Problem \ref{prob:LU} and another array representing the right hand side of a linear system and modifies the second array in place so that it represents the solution to the linear system.
No changes to the array storing the LU decomposition are necessary.
\end{problem}

\begin{problem}
\label{prob:det}
Write a function which uses the solution to Problem \ref{prob:REF} to find the determinant of $A$.
Notice that the solution to Problem \ref{prob:REF} computes $U$ when applied to a square matrix.
\end{problem}

SciPy includes more complete implementations of the LU decomposition in the \li{linalg} module.
This LU implementation returns an additional array that shows any row swaps that were made during the decomposition.
The LU decomposition is calculated with \li{lu}.
\li{lu_factor} calculates the LU decomposition and stores it in a single array.
It returns a tuple of arrays, one which is the LU decomposition and the other of which represents the necessary permutation of the rows.
\li{lu_solve} takes the output from \li{lu_factor} and uses it to solve a linear system.
Using \li{lu_factor} and \li{lu_solve} is a good alternative to inverting large matrices since it is faster to compute an LU decomposition than it is to invert a matrix.

\section*{The Cholesky Decomposition (Optional)}

For certain circumstances, we have a more efficient alternative to the LU decomposition.
The Cholesky decomposition requires half the number of calculations and half the memory that the standard LU decomposition needs.
Furthermore, it is a numerically stable decomposition, which makes it all the more useful.
Because of the efficiency and numerical stability, Cholesky decomposition is used in solving least squares, optimization, and state estimation problems.
The Cholesky decomposition, however, is only applicable to Hermitian (for real matrices, Hermitian is equivalent to symmetric) positive definite matrices.
In fact, the Cholesky decomposition is an efficient way to test if a matrix is positive definite.
The Cholesky decomposition of a positive definite matrix is unique.
Think of the Cholesky decomposition as the matrix equivalent taking the square root of a positive real number.

The Cholesky decomposition of a $A$ is an lower-triangular matrix, $L$, such that
\begin{equation*}
 A = LL^*
\end{equation*}
Where $L^*$ is the conjugate transpose of $L$.
For real valued matrices, this is equivalent to $L^T$.

The entries of $L$ are calculated as follows.
\begin{align*}
&L_{i,j} = \frac{1}{L_{j,j}}\left(A_{i,j} -\sum_{k=1}^{j-1}{L_{i,k}L_{j,k}^*}\right) \mbox{ for $i>j$} \\ \\
&L_{i,i} = \sqrt{A_{i,i} - \sum_{k=1}^{i-1}{L_{i,k}L_{i,k}^*}}
\end{align*}
where $L^*$ denotes the conjugate transpose of $L$.

Notice that in this computation, current calculation will depend on previous calculations.
To calculate $L$ properly, you must start in the upper left corner and iterate down.

Note: when testing positive definite systems, an easy way to generate a random symmetric positive definite matrix is by generating a random array \li{A} and then computing \li{A.dot(A.T)}.

\begin{problem}
Write your own implementation of the Cholesky decomposition.
Test it using a random symmetric matrix (build a random square matrix $A$, then $A^TA$ will be positive definite).
Check the output of your function to ensure that it is functioning properly.
\end{problem}

\begin{problem}
Modify your previous answer so that it computes the Cholesky decomposition by modifying the array in place.
Make sure you set the portion of the array above the main diagonal to 0.
Then write a function that takes this reduced form of the array and uses it to solve a linear system by back substitution.
This should be nearly the same as Problem \ref{prob:lusolve}.
\end{problem}

the linalg module of SciPy also includes a cholesky decomposition that should be much faster than the one you just computed.
It works much like the LU decomposition.
\li{cho_factor} returns the cholesky decomposition of the array that can then be used in solving a linear system with \li{cho_solve}.
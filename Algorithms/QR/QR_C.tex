\lab{Algorithms}{Modified Gram-Schmidt (QR)}{QR decomposition}
\label{lab:QRdecomp}

\objective{Understand how the QR algorithm works and write your own implementation.}

The QR decomposition is used to represent any matrix as the multiple of an orthogonal matrix and an upper triangular matrix. This decomposition is useful in computing least squares and is part of a common method for finding eigenvalues.

\section*{Review of Gram Schmidt}

\vspace{5mm}
\begin{theorem}[Gram-Schmidt Orthogonalization Process] Let
$\{\x_i\}_{i=1}^n$ be a basis for the inner product space $V$. Let
\[
\q_1 = \frac{\x_1}{\norm{\x_1}},
\]
and define $\q_2,\q_3,\ldots,\q_n$ recursively by
\[
\q_{k+1} = \x_{k+1} - \sum^k_{j=1}\frac{\langle \x_{k+1}, \q_j\rangle}{\norm{\q_{j}}^2}\q_j,
\]

the sum term is a projection of $\x_{k+1}$ onto the subspace $\mbox{Span}(\q_1,\q_2,\ldots,\q_k)$.  Then the set $\{\q_i\}_{i=1}^n$ is an orthonormal basis for $V$.
\end{theorem}
\vspace{5mm}

For the above algorithm, let $r_{j k} = \langle \x_k, \q_j\rangle$ when $j \leq k$.  Then
\begin{align*}
r_{1 1} \q_1 &= \x_1\\
r_{k k} \q_k &= \x_k - r_{1 k} \q_1 - r_{2 k} \q_2 - r_{3 k} \q_3 -
\ldots - r_{k-1, k} \q_{k-1},\quad k=2,\ldots,n.
\end{align*}
This can be written as
\begin{align*}
\x_1 &= r_{1 1} \q_1\\
\x_2 &= r_{1 2} \q_1 + r_{2 2} \q_2\\
\vdots \:\: &= \quad \vdots\\
\x_n &= r_{1 n} \q_1 + r_{2 n} \q_2 + \ldots + r_{n n} \q_{n},
\end{align*}
or in matrix form as
\[
\begin{pmatrix}
\vdots & \vdots & & \vdots\\
\x_1 & \x_2 & \cdots & \x_n\\
\vdots & \vdots & & \vdots
\end{pmatrix}
=
\begin{pmatrix}
\vdots & \vdots & & \vdots\\
\q_1 & \q_2 & \cdots & \q_n\\
\vdots & \vdots & & \vdots
\end{pmatrix}
\begin{pmatrix}
r_{1 1} & r_{1 2} & \cdots & r_{1 n}\\
0 & r_{2 2} & \cdots & r_{2 n}\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & r_{n n}
\end{pmatrix}.
\]
Hence if our original basis $\{\x_i\}_{i=1}^n$ correspond to column
vectors of a matrix $A$, we can likewise write the resulting
orthonormal basis $\{\q_i\}_{i=1}^n$ as a matrix $Q$ of column
vectors.  Then we have that $A = Q R$, where $R$ is the above
nonsingular upper-triangular $n\times n$ matrix.  This is the QR
Decomposition and is summarized by the following theorem:
\vspace{5mm}
\begin{theorem}
Let $A$ be an $m\times n$ matrix of rank $n$.  Then $A$ can be
factored into a product $Q R$, where $Q$ is an $m\times n$ matrix
with orthonormal columns and $R$ is a nonsingular $n \times n$ upper
triangular matrix.
\end{theorem}


There are three mode options available in SciPy's implementation of QR Decomposition.  We will be using the ``economic" option.
\begin{lstlisting}[style=python]
: import scipy as sp
: from scipy import linalg as la
: A = sp.randn(4,3)
: Q, R = la.qr(A, mode='economic')
: sp.dot(Q, R) == A     there will be some False entries
: sp.dot(Q, R) - A
: sp.dot(Q.T, Q)
\end{lstlisting}

In order to interpret the results correctly, we need to understand that the computer has limited precision (especially with floating point numbers).  This is why \li{sp.dot(Q, R)} is not exactly equal to A.  But subtracting the two yields numbers that are essentially zero.  This shows that indeed the product of $Q$ and $R$ is $A$.  Note also that $Q^T Q = I$.  This implies that the column vectors of $Q$ are orthonormal (why?).

\section*{Solving Least Squares Problems}

For large or ill-conditioned problems, the QR decomposition provides
a nice method for computing least squares solutions of
over-determined matrices.  Consider the problem $A x = b$.  Recall
that the least squares solution is $\widehat x = (A^T A)^{-1}A^T b$.
Alternatively, we write the linear system as
\[
Q R x = b.
\]
We then multiply both sides by $Q^T$, yielding
\[
R x = Q^T b.
\]
Then $\widehat x = R^{-1} Q^T b$.

\section*{Computational Remark}

Numerically, the Gram Schmidt process can have problems due to
finite precision arithmetic. Specifically, due to rounding errors,
the resulting basis may not be orthonormal. To combat this, we
actually carry out a slightly revised algorithm called Modified Gram
Schmidt.  To do this, we compute $\q_1$ as before.  We then project
it out of each of the remaining original vectors
$\x_2,\x_3,\ldots,\x_n$ via
\[
\x_k := \x_k - \langle \x_{k}, \q_1\rangle \q_1,\quad k=2,\ldots,n.
\]
Then we compute $\q_2$ to be the unit vector of $\x_2$, that is,
\[
\q_2 = \frac{\x_2}{\|\x_2\|}.
\]
We repeat by projecting out $\q_2$ from the remaining vectors
$\x_3,\x_4,\ldots,\x_n$.

\begin{problem}
\label{prob:QR}
Write your own implementation of the QR decomposition.  It should accept as input a matrix $A$ and computes its QR decomposition, returning the matrices $Q$ and $R$. Be sure to use the numerically stable Modified Gram Schmidt algorithm.
\end{problem}

\lab{Algorithms}{Floating Point Numbers}{Floating Point Numbers}
\label{lab:IEEE}
\objective{Gain a basic understanding of the IEEE floating point standard.}

\section*{Introduction}

Floating point numbers permeate modern computing, but it is not always good to use them without knowledge of what they are and how they work.
A fundamental understanding of the inner workings of floating point numbers can be very helpful when working with a wide variety of floating point computations.
By far the most common floating point standard is the IEEE floating point standard described by in the standards IEEE-754-1985, IEEE-854-1987, and IEEE-754-2008.
These standards outline how floating point numbers are represented in binary form and how operations like addition, subtraction, multiplication, division, and rounding should work.
Most floating point numbers are either 32 or 64 bits long.
There are also 16, 128, and 256 bit versions, but they are much less common.
Here we will consider 32 bit floating point numbers.

Before considering the details of IEEE floating point representation, it will be good to establish what a floating point number is at all.
Floating point representation is a way of representing a number that is similar to the "scientific notation" commonly used to represent numbers that are extremely large or small.
In general, a floating point number is a number written in the form $d \times \beta^p$ where $\beta \in \mathbb{N}$ is the base used for computation, $d$ is a decimal (like 1.10110) with base $\beta$ and $p\in \mathbb{Z}$ is the power of $\beta$.
The floating point representation of a number allows representation of real numbers, particularly when they are extremely large, or when they are not integers.
The normal mathematical operations on real numbers apply in the theoretical context.

In real applications, floating point numbers can be used to make an \textit{approximate} representation of \textit{a useful number} of real numbers.
There are uncountably many real numbers, so it is impossible to create a computer system that can distinguish perfectly between all of them, but that is okay since measurements are imperfect and approximate computations are usually good enough.

\section*{Binary Representation}

A 32 bit floating point number is represented by 32 bits, each storing either a 0 or a 1.

\lab{Algorithms}{Floating Point Numbers}{Floating Point Numbers}
\label{lab:IEEE}
\objective{Gain a basic understanding of the IEEE floating point standard.}

\section*{Introduction}

Floating point numbers permeate modern computing, but it is not always good to use them without knowledge of what they are and how they work.
A fundamental understanding of the inner workings of floating point numbers can be very helpful when working with a wide variety of floating point computations.
By far the most common floating point standard is the IEEE floating point standard described by in the standards IEEE-754-1985, IEEE-854-1987, and IEEE-754-2008.
These standards outline how floating point numbers are represented in binary form and how operations like addition, subtraction, multiplication, division, and rounding should work.
Most floating point numbers are either 32 or 64 bits long.
32 bit floating point numbers are known as "single precision floating point numbers" and 64 bit floating point numbers are known as "double precision floating point numbers."
There are also 16, 128, and 256 bit versions, but they are much less common.
Floating point numbers in Python are stored as double precision values.
Here we will consider 32 bit floating point numbers.

Before considering any more of the details of IEEE floating point representation, it will be good to establish what a floating point number is at all.
Floating point representation is a way of representing a number that is similar to the "scientific notation" commonly used to represent numbers that are extremely large or small.
In general, a floating point number is a number written in the form $d \times \beta^p$ where $\beta \in \mathbb{N}, \beta \geq 2$ is the base used for computation, $d$ is a decimal (like 1.10110) with base $\beta$ and $p\in \mathbb{Z}$ is the power of $\beta$.
Theoretically, for a given base $\beta \in \mathbb{N}, \beta \geq 2$, any real number $r \in \mathbb{R}$ can be represented at least one way by a series of the form
\begin{equation*}
r = d_0 \beta^{p} + d_1 \beta^{p-1} + \dots = \sum_{i=0}^{\infty} d_i \beta^{p-i}
\end{equation*}
where the $d_i$ are the digits of a decimal number $d$.
This representation is not necessarily unique, but finite approximations of this sort of sum can be useful in performing actual computations.
The floating point representation of a number allows easy representation of real numbers, particularly when they are extremely large, or when they are not integers.
The normal mathematical operations on real numbers apply in the theoretical context.

In real applications, floating point numbers can be used to make an \textit{approximate} representation of \textit{a useful number} of real numbers.
There are uncountably many real numbers, so it is impossible to create a computer system that can distinguish perfectly between all of them, but that is okay since measurements are imperfect and approximate computations are usually good enough.

\begin{problem}
\begin{itemize}

\item Make a Python class representing an arbitrary precision floating point number.
This shouldn't be too hard since Python's integers are arbitrary precision as well, just store one integer representing the exponent in some way and another representing the significand.
Make methods to do all of the following:
	\begin{itemize}

	\item Convert a floating point object to a Python float.

	\item Print the floating point number correctly.

	\item Copy the floating point number.

	\item Perform addition of two of your floating point objects (perform all operations without any sort of rounding first).

	\item Perform subtraction of two of your floating point objects.

	\item Perform multiplication of two of your floating point objects.

	\item Truncate the significand of the floating point number to remove all digits below a given power of 10.

	\end{itemize}

\item Now, make another class using the class you just wrote that tracks errors in computation.
Track two values, one for the exact value, and one for the exact error.
Computations involving the value and error should be carried out like they would be for your arbitrary precision floating point class.
Have it track the exact value of the significand and the error until you have it truncate all digits that fall within the error.
Make methods that do all of the following:
	\begin{itemize}

	\item Print the floating point number correctly.

	\item Copy the floating point nubmer.

	\item Perform addition of two of your floating point objects.
		Be sure to add the corresponding errors as well to represent that the possible range of values has increased.

	\item Perform subtraction of two floating point objects.
		Again, be sure to add the corresponding errors.

	\item Perform multiplication of two floating point objects.
		Here's how you can calculate the error term:
		Given two floating point numbers $a\pm\epsilon$ and $b\pm\delta$ where $a$ and $b$ are the values and $\epsilon$ and $\delta$ are the error terms, the result will be $ab \pm a\delta \pm b\epsilon \pm \epsilon\delta$.
		The first term in the expanded product will be the significand of the product and the rest will be the new error term.
		Be sure to take absolute values of $a$ and $b$ when calculating the error so that there is never a negative error term.

	\item Truncate the floating point number to the smallest power of ten that is larger than the error term.
		Set the new error term to be this same power of ten.
		(If you want to do many different calculations involving these floating point numbers, you may, at times have to truncate the values to prevent the integer storing the significand from becoming too large.)

	\end{itemize}

\end{itemize}
\end{problem}

\section*{Binary Representation}

A 32 bit floating point number is represented by 32 bits, each storing either a 0 or a 1.

The first bit in the binary representation of a floating point number represents the sign of the number.
If it is 0, the number is positive.
If it is 1, the number is negative.

The next 8 bits are used to store the exponent of the floating point number.
The exponent does not have an explicit sign bit, but is instead scaled so that the counting runs from -127 to 128.

The final 23 bits are used to store the binary decimal number.
This portion of the number is often called the "mantissa" or the "significand".
Since the first number of any decimal in binary is always one, the first number in the significand is assumed to be one and the bits included in the significand are used to represent the remaining digits.

For example, the following is the binary representation of the number .25.

\begin{equation*}
\underbrace{0}_{sign} \underbrace{01111101}_{exponent}\underbrace{(1.)}_{Implied 1.} \underbrace{00000000000000000000000}_{significand}
\end{equation*}

Since there is a limit on the number of digits used to represent the significand, rounding must occur at each step.
There are several possible rounding conventions, but IEEE floating point rounds the significand to the nearest even value, which in binary means that the last digit must become 0.
For example, using binary decimals, the value $1011011.1$ would round to $1011100$ since that is the nearest even number.

Since rounding occurs after each operation, addition, subtraction, multiplication, and division behave \textit{approximately} how they should.
In some cases, the usual properties you would expect from these operations do not hold.
More on that later.

For a double precision floating point number, there is still one bit to store the sign.
There are 11 bits to store the exponent, and 52 bits used to store the value (with the one still implied).

Floating point operations on modern processors are heavily optimized.
There are built in operations that allow for addition, subtraction, multiplication, division, square roots, and several other operations.
Floating point operations are actually a common way of measuring the performance of computer systems.
What is considered a single floating point operation depends somewhat on the system.
Some processors can perform a single addition and multiplication in a single clock cycle, so a floationg point operation is sometimes considered to be an addition and a multiplication.
It can also be measured as an addition or a multiplication.

The highest and lowest values in the exponent are reserved for modified numbers.
Zero is represented using the smallest exponent value (binary 0, representing a negative power of 2 since the exponent is scaled) with a significand of 0.
When the exponent is as small as possible, the significand is no longer considered normalized and the implied one is no longer considered.
This allows for the representation of even smaller numbers, but it implies the loss of some precision.
For example, the following code in Python yields 0
\begin{lstlisting}
1.123456789012345 * 10.**-306 * 10.**153 * 10.**153 - 1.123456789012345
\end{lstlisting}
While this yields -0.0365123681616.
\begin{lstlisting}
1.123456789012345 * 10.**-322 * 10.**161 * 10.**161 - 1.123456789012345
\end{lstlisting}
Numbers represented in this way are called "denormalized numbers" or "subnormal numbers."
Floating point zero is also still allowed to have a sign, so for example, \li{-0.} is valid in Python and prints as \li{-0.0}.
\li{0.} is considered equal to \li{-0.}, but the sign still carries through in multiplication, so for example \li{-0. *  0.} is \li{-0.}.
There are no analogues of the denormalized numbers.
The values of the highest exponent are used to store different kinds of infinity and "nan" values (nan stands for "not a number").
IEEE floating point allows for both negative and positive infinity.
\li{nan} values are defined to never be equal to one another.
Positive and negative infinity can be used for equality testing, for example, the following returns \li{True}.
\begin{lstlisting}
a = (10.**300)
a *= a # a is now positive infinity
a == a
\end{lstlisting}

% see how they measure it at http://software.intel.com/en-us/articles/estimating-flops-using-event-based-sampling-ebs




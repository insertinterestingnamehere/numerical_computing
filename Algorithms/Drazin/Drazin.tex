\lab{Algorithms}{Drazin Inverse}{Drazin Inverse}

\objective{This section explains methods for computing the Drazin Inverse}

In the last section we talked about the Moore-Penrose inverse. Much of the power of the Moore-Penrose Inverse is derived from it's equation solving ability (it gives the least-square solution). However, by doing so, it sacrifices other properties that we expect a matrix to have.

For example the Moore-Penrose Inverse of a Matrix $A$ generally will not satisfy the relations
\begin{equation} \label{eq:DrazinOne}
A A^\dagger = A^\dagger A
\end{equation}

\begin{equation} \label{eq:DrazinTwo}
A^{p+1} A^\dagger = A^p
\end{equation}

\begin{equation} \label{eq:DrazinThree}
\lambda \in \sigma(A) \iff \lambda^{-1} \in \sigma(A^\dagger)
\end{equation}

These properties are all very desirable in certain settings, but the Moore-Penrose Inverse fails to have them generally. Therefore, we develop a different type of inverse that does have these properties: the Drazin Inverse.

First, we define the index of a matrix. The index is the smallest integer $k$ such that $Rank(A^k) = Rank(A^{k+1})$. We then define the Drazin inverse, $A^D$, as the matrix that satisfies the following:

\begin{equation} \label{eq:DrazinCondOne}
A A^D = A^D A
\end{equation}

\begin{equation} \label{eq:DrazinCondTwo}
A^{k+1} A^D = A^k
\end{equation}

\begin{equation} \label{eq:DrazinCondThree}
A^D A A^D = A^D
\end{equation}

We note, first of all, that the Drazin inverse is only defined on square matrices (in fact, the index of a matrix is only defined on square matrices). We also note that the properties \ref{eq:DrazinOne} -\ref{eq:DrazinThree} are basically satisfied by the Drazin inverse, as desired.

{\bf Again, I'm not sure how much Jeff is covering in the book, so I'm not sure exactly how much exposition and exercises to give.}
\begin{problem}
Show that the Drazin Inverse of a nilpotent matrix is the zero matrix.
\end{problem}

\begin{problem}
Show that the Drazin Inverse of a projection matrix is itself.
\end{problem}


One method of calculating the Drazin Inverse is as follows:

Define a sequence of full-rank factorizations as follows: $A = B_1 C_1$, $C_1 B_1 = B_2 C_2$, $C_2 B_2 = B_3 C_3$ etc. We require that these full-rank factorizations be such that $B_i$ be a $p \times q$ matrix, where $p$ is the size of $C_{i-1} B_{i-1}$ and $q$ be the rank of that matrix. Note that in each case $B_i$ and $C_i$ have the same rank, and that the matrix $C_i B_i$ will get progressively smaller. This process is repeated until the matrix $C_k B_k$ is either invertible or is zero. We then have the following:

\[
\text{ind}(A) = \left\{
\begin{array} {l l}
i & \quad \text{if  $(C_i B_i)^{-1}$ exists}\\
i+1 & \quad \text {if  $(C_i B_i) = 0$}\\
\end{array} \right.
\]

\[
A^D = \left\{
\begin{array} {l l}
B_1 B_2 \ldots B_k (C_k B_k)^{-(k+1)} C_k \ldots C_2 C_1 & \quad \text{if } (C_k B_k)^{-1} \text{ exists}\\
0 & \quad \text {if } (C_i B_i) = 0 \\
\end{array} \right.
\]

This definition doesn't hold exactly as stated when $A$ is an invertible matrix (since the index of an invertible matrix is zero) but in that case the process would reveal this issue quickly (since $B_1$ and $C_1$ would be the same size as $A$).

The natural consideration is now how to construct the full-rank factorization. This can be done using the SVD. For example, the desired full rank factorization of the matrix A, in Python code, will be:


\begin{lstlisting}
U,s,Vh = la.svd(A)
S = sp.diag(s)
S = S*(S>tol)
r = sp.count_nonzero(S)
B = sp.dot(U,sp.sqrt(S))
C = sp.dot(sp.sqrt(S),Vh)
B = B[:,0:r]
C = C[0:r,:]
\end{lstlisting}

This code calculates the SVD of $A$, and then identifies the (numerical) rank using a specified tolerance. It then builds the full-rank factorization using the square root of $S$, which is $\Sigma$ in standard notation. We then have to trim off the columns of $B$ and the rows of $A$ that are zero.

\begin{problem}
Write a function to calculate the Drazin Inverse using this method. Allow the user the option of inputting a tolerance. We give the following to help you test:
\[
P = \begin{pmatrix}
1 & 0 \\
0 & 0 
\end{pmatrix} \quad P^D = \begin{pmatrix}
1 & 0 \\
0 & 0 
\end{pmatrix}
\]

\[
A = \begin{pmatrix}
10 & -8 & 6 & -3 \\
12 & -10 & 8 & -4 \\
1 & -1 & 1 & 0 \\
-2 & 2 & -2 & 2
\end{pmatrix} \quad A^D = \begin{pmatrix}
2 & -3/2 & 1 & -1/2 \\
2 & -3/2 & 1 & -1/2 \\
-1 & 1 & -1 & 1 \\
-2 & 2 & -2 & 2
\end{pmatrix}
\]

\[
N = \begin{pmatrix}
0&1&0&0&0 \\
0&0&1&0&0 \\
0&0&0&1&0 \\
0&0&0&0&1 \\
0&0&0&0&0
\end{pmatrix} \quad N^D = \begin{pmatrix}
0&0&0&0&0 \\
0&0&0&0&0 \\
0&0&0&0&0 \\
0&0&0&0&0 \\
0&0&0&0&0
\end{pmatrix}
\]
\end{problem}

We note that the Drazin inverse can be very sensitive, particularly when it has eigenvalues close to zero.

%Obviously, gallery(5) is not in Python. However, the matrix is small so we could just print it here
\begin{problem}
Use your function to calculate the Drazin Inverse of the matrix below.
\[                                                          
\begin{pmatrix}
-9&11&-21&63&-252 \\
70&-69&141&-421&1684 \\
-575&575&-1149&3451&-13801 \\
3891&-3891&7782&-23345&93365 \\
1024&-1024&2048&-6144&24572 \\
\end{pmatrix}
\]                                                           What do you get? This matrix is actually nilpotent, and so the Drazin Inverse should be zero. Find a tolerance at which the correct answer is attained.
\end{problem}






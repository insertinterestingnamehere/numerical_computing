\def\x{{\bf x}}
\def\q{{\bf q}}
\def\p{{\bf p}}
\def\v{{\bf v}}

\lab{Algorithms}{Fast Fourier Transform}{Fast Fourier Transform}
\objective{This lesson explores the fourier transform applied to audio signals.}

\section*{Audio Signals}

In this lab we will working with signals which represent sound. First we should understand how sound is produced and transmitted. As an example, consider a typical electronic speaker.

\begin{center}\includegraphics[width=45mm]{speaker}\end{center}

The essential component is a bowl-shaped membrane, called the \emph{cone} (or \emph{diaphragm}) of the speaker. Behind the cone is attached a lightweight electromagnet. When current flows through the electromagnet in a certain direction, a magnetic field is created which repels the electromagnet (and the attached cone) from a  heavier permanent magnet mounted behind it. The cone is thus thrust forward, compressing the air in front of the speaker, resulting a slight increase in air pressure which is propagated through the room in a wave. When the direction of the current is reversed, the electromagnet is attracted to the permanent magnet. The cone is thus pulled back, expanding the air in front of the speaker, resulting in a slight decrease in air pressure which, again, is propagated through the room in a wave. These pressure waves reach your ears and are perceived as sound. The basic mechanism of a speaker is entirely reversible; for instance, a pair of ordinary (non-amplifying) headphones can also, without any modification, be used as a stereo microphone (Try it!).

Play the audio clip \texttt{pulseramp.wav}. You will hear a series of pulses, or clicks. A click is simply a sudden (small) increase in pressure followed almost immediately by a return to normal pressure. By a speaker, a click is produced by the cone suddenly thrusting forward a small amount and then gradually, silently, returning to its original position. Notice that as the pulses occur closer and closer together, eventually your ears no longer perceive them as individual pulses. Instead they are perceived as a tone which increases in pitch as the pulses increase in frequency.

In python we can read in a \texttt{.wav} by importing wavfile from scipy.io and using wavfile.read() as follows
\begin{lstlisting}
from scipy.io import wavfile
rate, mywave = wavfile.read('pulseramp.wav')
\end{lstlisting}
Here \texttt{data} is the signal and \texttt{rate} is the sampling rate of the audio measured in Hz.  We can check the sampling rate and the size of the signal:
\begin{lstlisting}
rate
Out[2]: 44100

mywave.shape
Out[3]: (300000,)
\end{lstlisting}
We find that \texttt{mywave} is a vector of length $300000$. It has only one column because this is a mono (single-channel) \texttt{.wav} file. If we had used a stereo \texttt{.wav} file, then mywave would have been a matrix with columns, one for the left channel and one for the right.

The entries of \texttt{mywave} are integers between $-32767$ and $+32767$ which represent the relative air pressure at a particular instant, sampled at regular intervals (these bounds are the range of 16-bit integers).   In our case, the \texttt{.wav} file is sampled at 44.1KHz (the standard sample-rate for high-quality audio), so there are 44100 samples per second. $+32767$ represents the maximum possible increase in air pressure which the speaker can produce (at its current volume level), while $-32767$ represents the maximum possible decrease in air pressure. In the case of our particular \texttt{.wav} file, you can check that most of the entries of \texttt{mywave} are $0$, indicating neutral pressure. Only occasionally you will find an entry which is $+32767$, which is where a pulse occurs.
We can see this most easily if we plot the waveform of our sound (see Figure \ref{pulseramp}):
\begin{lstlisting}
from matplotlib import pyplot as plt
plt.plot(mywave)
plt.show()
\end{lstlisting}
\begin{figure}[ht]\caption{A sequence of pulses of increasing frequency}\label{pulseramp}\centering\includegraphics[width=\textwidth]{pulseramp}\end{figure}

We should keep in mind that the deviations in air pressure that we hear as sound are extremely small relative to atmospheric pressure. For instance, in normal conversation, the deviation in air pressure reaching your ears might be around $.01$ pascals, about one ten-millionth of atmospheric pressure.

\begin{problem}
Modify the sound loaded from \texttt{pulseramp.wav} so that the pulses are bursts of negative pressure instead of positive pressure, i.e., change each +32767 to -32767. (Hint: This can be done in one line. Think of \texttt{mywave} as a vector.) Save the resulting sound to a file using the \texttt{wavfile.write} function which has the following syntax

\begin{lstlisting}
wavfile.write('negpulseramp.wav',rate,neg_data)
\end{lstlisting}
where the first entry is the name you want to save the new file as (don't forget the file extension), the second entry is the sampling rate, and the third entry is the data.

Be sure to use a sample rate of 44100Hz. Then play the new \texttt{.wav} file. Does it sound different from the original?
\end{problem}

\section*{Using the Fourier Transform to Investigate the Spectrum of a Signal}

In many ways the most basic kind of sound is one whose waveform is a sinusoid (i.e., a scaled, shifted version of the sine function). The tone of a sinusoid is, to the ear, more mellow than any other sound of the same pitch. We have seen that, mathematically, sinusoids are important, because Fourier series provide a way of expressing any periodic, piecewise continuous function as a sum of sine and cosine waves of various frequencies and amplitudes, or, more conveniently, using complex exponential functions.

Here's one way of producing a a 1760Hz sine wave (which in musical terms is A6, the A between two and three octaves above middle C):

\begin{lstlisting}
import scipy as sp
from scipy.io import wavfile

samplerate=44100 # 44100 samples per second
freq=1760 # We're going to produce a 1760 Hz sine wave ...
length=2 # ... which will last for 2 seconds.
stepsize=freq*2*sp.pi/samplerate
sig=sp.sin(sp.arange(0,stepsize*length*samplerate,stepsize))
scaled = sp.int16(sig/sp.absolute(sig).max() * 32767)
wavfile.write('sinusound.wav',samplerate, scaled)
\end{lstlisting}

Notice before writing the file we had to scale it to be a 16-bit integer between -32767 and +32767 because wavfile.write expects the data to be in this form.  Now play this sound.

\begin{problem}
Produce a 60Hz sine wave 1 second in length. Then plot the signal. Play the sound. Is it audible?
\end{problem}


For a discrete signal, we are given finitely many data points (assumed to be evenly spaced on the $x$-axis); we can think of such a signal as a function $f$ whose domain consists of the integers $\{0,1,\dots,N-1\}$, where $N$ is the number of data points. For convenience, we will often extend $f$ to be a periodic function with period $N$, defined on all integers, by defining $f(a+bN)=f(a)$ for all integers $a$ and $b$. How might we find something analagous to a Fourier series for such a function? Well, it turns out that the functions, $e^{\frac{2\pi ikx}N}$, which are periodic with period $N$, form an orthonormal set with respect to the inner product
$$\langle f,g\rangle = \frac1N\sum_{k=0}^{N-1}\overline{f(k)}g(k)$$
In particular, these functions are linearly independent and hence form a basis for the $N$-dimensional (complex) vector space of complex-valued functions with domain $\{0,\dots,N-1\}$. The vector of coordinates of a function $f$ with respect to this basis constitutes what is called the \emph{Discrete Fourier Transform} (or \emph{DFT}) of the function $f$. In other words, any function $f$ whose domain consists of the integers $\{0,1,\dots,N-1\}$ may be expressed
$$f(x)=\sum_{k=0}^{N-1}c_ke^{\frac{2\pi ikx}N}$$
and we call the vector of coefficients $(c_0,c_1,\dots,c_{N-1})$ the DFT of $f$. Explicitly, these coefficients may be computed by taking the inner product of each basis function with $f$:
$$c_k=\langle e^{\frac{2\pi ikx}N}, f(x)\rangle=\frac1N\sum_{x=0}^{N-1}e^{\frac{-2\pi ikx}N}f(x)$$

In scipy, there is a module \texttt{scipy.fft} that contains functions for computing the DFT of a signal. (\texttt{fft} stands for Fast Fourier Transform.) For example, if we create a sine wave signal \texttt{sig} as above and then type,
\begin{lstlisting}
from scipy import fft
fsig = fft(sig)
\end{lstlisting}
then \texttt{fsig} is an array of complex numbers representing the DFT of this signal. We will plot the absolute values of the entries of \texttt{fsig} (see Figure \ref{sinespec}):


\begin{problem}
Implement a function to compute the DFT of a vector using the naive algorithm, directly from the definition of the DFT. Test it on several random vectors and make sure the results match those of scipy's \texttt{fft}.  Note that the scaling scipy uses does not divide by $N$, so you will need to account for that in comparing your results.
\end{problem}

The naive way of computing the DFT has $O(N^2)$ runtime.  There is a very important algorithm (and many variations of it) called the Fast Fourier Transform (FFT) which computes the DFT in $O(N \log N)$ runtime.  Essentially the FFT algorithm accomplishes this speedup by breaking the recursively breaking up the original array into smaller pieces, finding the Fourier Transform of those smaller pieces, and carefully recombining to get the Fourier Transform of the original array.  While scipy uses the FFT algorithm, it's implementation at the time of this writing is somewhat slow.  

There is a library written in $C$ for computing the FFT called FFTW (Fastest Fourier Transform in the West) that is substantially quicker.  There is a python package available for download called ``anfft'' that wraps fftw in python.  Download and install it for use in the remaining problems.

\begin{figure}[ht]\caption{DFT of a sine wave}\label{sinespec}\centering\includegraphics[width=\textwidth]{sinespec}\end{figure}
Notice that there are two spikes, one on the left and one on the right. Why are there two spikes when there is only one tone? The answer is that for a real-valued signal, the right half of the DFT is always a mirror-image duplicate of the left half -- or, more precisely, each entry in the right half of the DFT is the complex conjugate of the corresponding entry in the left half. Thus the right half of the DFT gives us no new information, and we may as well ignore it.

In saying that the right half of the DFT is a complex-conjugate mirror-image of the left half, to be perfectly correct we actually need to ignore the very first entry of the DFT.

Now let's look at a more complicated signal. Play the file \texttt{tada.wav}.  If you read \texttt{tada.wav} into python you will find it has two columns.  That is because it is a stereo recording meaning the two columns correspond to two audio signals (left and right).  When using anfft, it will take the fourier transform along the last axis.  Because of this we will take the transpose before computing the FFT and again after to return to the original shape.  Also note that anfft expects signals to have scipy's float16 or float32 as the datatype.  Consequently after reading in the signal we cast it as a float32. 

Now we plot the DFT of \texttt{tada.wav} (see Figure \ref{tadaspec}):
\begin{lstlisting}
import scipy as sp
from scipy.io import wavfile
import anfft

rate, sig = wavfile.read('tada.wav')
sig = sp.float32(sig)
fsig = anfft.fft(sig.T).T
plt.figure()
plt.plot(sp.absolute(fsig))
plt.show()
\end{lstlisting}


\begin{figure}[ht]\caption{Absolute value of the DFT of \texttt{Tada.wav}}\label{tadaspec}\centering\includegraphics[width=\textwidth]{tadaspec}\end{figure}
Again, the right half of the plot is a mirror-image duplicate of the left half. We might as well exclude it by only plotting the left half. The absolute value of the left half of  the DFT is called the \emph{spectrum} of the signal (see Figure \ref{tadaspec2}):
\begin{lstlisting}
f = sp.absolute(fsig)
plt.plot(f[0:f.shape[0]/2,:])
plt.show()
\end{lstlisting}
\begin{figure}[ht]\caption{Spectrum of \texttt{Tada.wav} (Absolute value of left half of DFT)}\label{tadaspec2}\centering\includegraphics[width=\textwidth]{tadaspec2}\end{figure}
Notice that the number of spikes in the plot is much greater than the number of musical notes in the sound. This is because when a one note is played on a musical instrument, more than one frequency is produced: the lowest frequency is called the \emph{fundamental}; the others, which usually occur as integer multiples of the fundamental frequency, are called \emph{overtones}. In instruments with a mellow tone (such as a flute or vibraphone) the overtones are relatively soft, while in instruments with a brighter tone (such as a trumpet) the overtones are quite prominent. In the DFT of \texttt{Tada.wav}, you will notice spikes at about 2030, 4060, 6090, 8120, 10160, 12190, 14220, 16250, 18280, and 20320 units. These are approximately integer multiples of 2030 and represent overtones of a single musical note whose fundamental frequency is about 2030 units.

Now what are these units and how can we express them in Hz? Well, a frequency of 2030 units in the DFT means that the tone undergoes 2030 cycles through the extent of the entire recording. We use check the shape of \texttt{sig} to find the total number of samples in the recording:
\begin{verbatim}
sig.shape
Out[27]: (42752, 2)
\end{verbatim}
So the entire recording is 42752 samples long (in each channel -- this is a stereo recording.) The sample rate is 22050 samples per second. (You could find this out, for instance, by right clicking on the \texttt{.wav}, selecting \emph{Properties} and looking under the \emph{Summary} tab).  Thus, 2030 cycles through the entire recording is equivalent to
$$\frac{2030\text{ cycles}}{42752\text{ samples}} \times \frac{22050\text{ samples}}{\text{sec}} \approx 1047 \text{ Hz}.$$
So this tone is at 1047 Hz, which in musical terms is a (concert) C6, the C two octaves above middle C. There are also other, lower musical notes in the recording which each have their own sequence of overtones.

\begin{problem}
Listen to \texttt{pianoclip.wav}. Identify the audible frequency of greatest amplitude. What are the frequencies of its overtones which are visible as spikes in the DFT? What musical note does the fundamental frequency correspond to? (For this, you may wish to use the table on http://en.wikipedia.org/wiki/Piano\_key\_frequencies)
\end{problem}

You will notice that in this case the largest spike actually occurs in the very first entry of the DFT. This corresponds to a frequency of 0Hz. A cosine wave of 0Hz is nothing other than a constant function. The presence of this 0Hz frequency, if taken literally, would mean that there was a (slight) sustained increase above the normal atmospheric pressure throughout the recording. However, a speaker or musical instrument, because of its bounded ranges of motion, is incapable of producing a sustained increase (or decrease) in pressure; and a microphone, for the same reason, is incapable of detecting such a sustained change in pressure. An ordinary speaker cannot produce frequencies below about 15 to 20Hz, and even if it could, we would not be able to hear them. The 0Hz frequency which appears here is simply an artifact of an imperfect recording device. It indicates that some direct current ``leaked" through the device.


\section*{Cleaning up a noisy signal}

Listen to \texttt{Noisysignal1.wav} (see Figure \ref{noisysignal}). This is a mono recording of a (probably familar) voice with some annoying noise over it. Fortunately for us, this noise is ``colored", i.e. it only occurs over a limited range of frequencies. This makes it possible for us to remove it fairly easily without unduly damaging the underlying signal.
\begin{figure}[ht]\caption{Noisy signal}\label{noisysignal}\centering\includegraphics[width=90mm]{noisy}\end{figure}
How do we begin? First we examine the DFT of the signal in order to identify which frequencies make up the noise (see Figure \ref{noisyspec}).
\begin{figure}[ht]\caption{Spectrum of noisy signal}\label{noisyspec}\centering\includegraphics[width=\textwidth]{noisyspec}\end{figure}
We notice a large band from 10000 to 20000 units which seems to be intruding in the spectrum. This is our noise. (The fact that it begins and ends so abruptly in the spectrum, instead of tapering off gradually, should be a clue that this noise was produced artificially, by the author of this lab, rather than arising in a natural way in the recording or transmission process.) To remove the noise, we simply zero out this part of the DFT and then take the inverse DFT (using the function \texttt{anfft.ifft}) to get our cleaned-up signal. But we must remember to zero out the corresponding part in the right half of the DFT as well, in order to ensure that the inverse DFT of the result is a real-valued signal.

It is critical that the values zeroed in the right half of the DFT be precisely the mirror images of the zeroed in the left half: i.e., assuming the DFT of the signal is stored in the vector \texttt{fsig}, if we set \texttt{fsig[j]} to zero then we should also set \texttt{fsig[-j]} to zero. If we are off on the index even just by one, then the result will not be a real valued signal and thus will not be playable.

So, we do the whole thing in Python like this:
\begin{lstlisting}
rate,data = wavfile.read('Noisysignal1.wav')
fsig = sp.fft(data,axis = 0)
for j in xrange(10000,20000):
    fsig[j]=0
    fsig[-j]=0

newsig=sp.ifft(fsig)
newsig = sp.real(newsig)
newsig = sp.int16(newsig/sp.absolute(newsig).max() * 32767)
\end{lstlisting}

Now we can either save the resulting cleaned-up signal \texttt{newsig} to a \texttt{.wav} file using \texttt{wavwrite}. If we plot it, the individual syllables are now visible in the waveform (see Figure \ref{cleansignal}).

\begin{figure}[ht]\caption{Cleaned-up \texttt{Noisysignal1.wav} }\label{cleansignal}\centering\includegraphics[width=\textwidth]{Cleanedsignal}\end{figure}

\begin{problem}
Listen to \texttt{Noisysignal2.wav}. You will probably just hear noise. However, there is a signal behind it. Remove the noise using the technique described above. In order to make the cleaned-up signal audible, you will probably need to amplify it. (\texttt{soundsc} does this automatically.) What does the voice say? Who is the speaker? (If you don't know the answer to this last question, try a quick Google search.)
\end{problem}

\section*{Down-sampling a signal}

Listen to \texttt{saw.wav}. You will hear a high-pitched buzzing sound \footnote{The technical term for it is a sawtooth wave; for the curious, see http://en.wikipedia.org/wiki/Sawtooth\_wave}. The sound is recorded at a sample rate of 44.1KHz. Now let's say we wanted to convert it to a 22.05KHz sample rate. How might we do that? Since we're cutting the sample rate in half, we might think that we could accomplish this simply by discarding every other sample, i.e., keep the even-index samples and discard the odd-index samples. Let's try that and see how it works:
\begin{lstlisting}
rate, in_sig = wavfile.read('saw.wav')
out_sig = in_sig[sp.arange(0,rate,2)]
wavfile.write('down_saw.wav',rate/2,out_sig)
\end{lstlisting}
What does it sound like? Well, it sounds much like the original except that there are some extra tones which weren't there to begin with; these extra tones, in this case, are not in a nice harmonic relationship with the original tone and clash rather badly. One of the extra tones is at a rather low frequency of about 99Hz compared to 1568Hz of the fundamental frequency in the original tone. Where are these mysterious ``extra" tones coming from?

To explain this, we need to understand some important facts about sampling. First of all, a basic fact is that the highest possible frequency that a sampled signal can represent is half the sampling rate. Such a frequency is represented by alternating between +32767 and -32767 at each sample. This frequency is known as the \emph{Nyquist frequency}. If, for instance, the sampling rate is 44.1KHz, then the Nyquist frequency will be 22.05KHz. This is slightly beyond the upper limit of human hearing; thus, for audio, generally no improvement in quality results from using a higher sampling rate than 44.1KHz, since at 44.1KHz the entire audible spectrum can already be represented.

Now let's consider what would happen if we tried to represent a frequency beyond the Nyquist frequency. We create a sinusoid of frequency $f$ Hz by setting
$$s_k = \cos\left(\frac{2\pi kf}r\right)$$
where $s_k$ is the value of the $k$th sample and $r$ is the sample rate in Hz. Suppose we take the frequency to be $f=\frac{r}2+a$, where $a$ is some positive number, so that $f$ is beyond the Nyquist frequency. Then we have
\begin{align*}
s_k &= \cos\left(\frac{2\pi kf}r\right) \\
&= \cos\left(\frac{2\pi k(\frac{r}2+a)}r\right)\\
&= \cos\left(\pi k+\frac{2\pi ka}r\right)\\
&= \cos\left(-\left(\pi k+\frac{2\pi ka}r\right)\right)\\
&= \cos\left(-\left(\pi k+\frac{2\pi ka}r\right)+2\pi k \right)\\
&= \cos\left(\pi k-\frac{2\pi ka}r\right)\\
&= \cos\left(\frac{2\pi k(\frac{r}2-a)}r\right).
\end{align*}
But this is a sinusoid of frequency $\frac{r}2-a$, which is a frequency below the Nyquist frequency. Thus a sinusoid above the Nyquist frequency, when sampled, is indistinguishable from a corresponding sinusoid below the Nyquist frequency. An electronic speaker, given the sampled signal, will actually produce the sinusoid which lies below the Nyquist frequency, not the one above. Thus,
\emph{A frequency above the Nyquist frequency will, when sampled, have its frequency reflected across the Nyquist frequency.}
This phenomenon is known as \emph{aliasing}.

What does this have to do with our problem? Our tone is at 1568Hz, which is well below the Nyquist frequency of 22050Hz. However, there are prominent overtones extending all the way up to 21951Hz (see Figure \ref{sawspec}). Since we are downsampling to 22050Hz, the new Nyquist frequency will be 11025Hz. The overtones in our signal which lie between 11025Hz and 22050Hz will then be reflected across the new Nyquist frequency in our downsampled sound. For example, the overtone at 21951Hz will be reflected to $11025-(21951-11025)=99$ Hz, the lowest ``extra" tone that we observed. See Figure \ref{sawspecdown}.

\begin{figure}[ht]\caption{Spectrum of \texttt{saw.wav}}\label{sawspec}\centering\includegraphics[width=\textwidth]{sawspec}\end{figure}

\begin{figure}[ht]\caption{Spectrum of \texttt{saw.wav} naively downsampled to 22050Hz}\label{sawspecdown}\centering\includegraphics[width=\textwidth]{sawspecdown}\end{figure}

So, how do we downsample our signal in a way that avoids this unwanted aliasing effect? What we do is take the DFT of our signal, cut out the upper half of the spectrum (which can no longer be represented, after we downsample), then take the inverse DFT of the result. It's as simple as that.

Well -- \emph{almost} that simple. Actually, when we say we're cutting out a set of frequencies from the spectrum, we mean we're removing those frequencies from the left half of the DFT and also the corresponding frequencies from the right half of the DFT. So, we're actually removing a big chunk out of the \emph{middle} of the DFT and then pasting the two ends back together. We can do this as follows where in\_sig is the signal from \texttt{saw.wav}:
\begin{lstlisting}
old_rate = 44100
new_rate = 22050
in_sig = sp.float32(in_sig)
fin = anfft.fft(in_sig)
nsiz = sp.floor(in_sig.size*new_rate/old_rate)
nsizh = sp.floor(nsiz/2)
fout = sp.zeros(nsiz)
fout = fout + 0j
fout[0:nsizh] = fin[0:nsizh]
fout[nsiz-nsizh+1:] = sp.conj(sp.flipud(fout[1:nsizh]))
out = anfft.ifft(fout)
\end{lstlisting}
There are just a couple minor problems we need to deal with before we're finished. First, because of roundoff error, you will find that some of the entries of the \texttt{out} have (slightly) nonzero imaginary components.  So we should first make sure the signal is purely real,
\begin{lstlisting}
out = sp.real(out) % Take the real component of the signal
\end{lstlisting}
Second, as always, we need to rescale and cast as a 16-bit integer:
\begin{lstlisting}
out = sp.int16(out/sp.absolute(out).max() * 32767)
\end{lstlisting}


\begin{figure}[ht]\caption{Spectrum of \texttt{saw.wav} correctly downsampled to 22050Hz}\label{sawspecdown2}\centering\includegraphics[width=\textwidth]{sawdownspec}\end{figure}

\begin{problem}
Convert the same sound, \texttt{saw.wav}, to a 11025Hz sampling rate. Do you notice a difference in the quality of the sound? If so, can you explain why?
\end{problem}

\begin{problem}
Can the technique above be used to change the sampling rate by something other than an integer factor, e.g., from 44100Hz to 36000Hz? Try it.
\end{problem}

\begin{problem}
Suppose we would like to upsample \texttt{Tada.wav} from a sampling rate of 22050Hz to 44100Hz. \begin{enumerate}
\item[(a)]
The naive way to do this would be to use linear interpolation on the original signal, i.e., the original signal would form the odd-index samples of the new signal, while the even-index samples of the new signal would be formed by taking the average of each two consecutive samples in the original signal. Write code to carry out the upsampling in this way; plot the spectrum of the result. What do you observe?
\item[(b)]Now write code to upsample the signal using the DFT, using a technique similar to what we did above for downsampling. Again, plot the spectrum of the result. Can you hear any difference between this and the result of (a)?
\end{enumerate}
\end{problem}

\section*{Filtering and Convolution}

We have already seen how the DFT can be used to filter out certain bands of frequencies from a signal. In this section we will look at how the DFT can be used to carry out another type of filtering effect. To begin, suppose we have a recording of musical piece played in, say, a small carpeted room with essentially no acoustics, and suppose we would like to apply an effect to make it sound as if the piece were played in a large concert hall (or some other room). How could we do that?

The first thing we need is a recording of the so-called \emph{impulse response} of the room whose acoustics we are trying to imitate. This is a recording of how the room responds to a short pulse of sound. Effective ways of producing a loud sound approximating a pulse include firing a (blank) gunshot, popping a balloon, or, if neither of those are available, clapping the hands one time. You have probably noticed (or can imagine) that if you pop a balloon in large room, although the sound of the actual pop only lasts a few milliseconds, you can hear the sound echoing about the room for up to several seconds. This echoing sound is called the impulse response of the room. Actually, it is somewhat imprecise to refer to \emph{the} impulse response of the room, since the recorded response can significantly depend both on the location of device producing the pulse and on location of the listener (or recording device) in the room.

Now, the idea is that if we know how the room responds to a pulse, then we can reconstruct how the room will respond to any sound. This is because any sound can be considered to be a series of pulses of varying amplitudes and polarity (i.e., they may be positive or negative). Each sample of sound can be considered as a pulse, even though the ear will not perceive it that way, because the pulses are so close together. Now, we make the assumption that the room's response is linear, meaning that a pulse of $k$ times the amplitude will produce a response of $k$ times the amplitude but otherwise identical; such an assumption is generally reasonably accurate, assuming the pulse is not extremely loud. To calculate how the room responds to a complex sound consisting of millions of samples, we simply add up its response to each sample of the sound, each such response simply being a scaled copy of the impulse response which we recorded.

Thus, in our simulation, each sample of our sound (of which there are usually 44100 per second) triggers a new scaled copy of the room's impulse response. If the impulse response is several seconds long, that means there may be a hundred thousand or more scaled copies of the impulse response sounding at once, which must all be mixed together to produce the final sound. This may be starting to seem computationally infeasible or at least very difficult. The key is to recognize that this process can be described as a convolution: namely, the final sound is simply the convolution of the our original sound with the impulse response. We can calculate convolutions quickly using the convolution theorem which says

\[\mathcal{F}(f \ast g) = (\mathcal{F} f) (\mathcal{F} g)\]

where $\mathcal{F}$ is the Fourier Transform and $\ast$ is convolution.

Thus we calculate the convolution of two arrays by simply taking the fourier transform of each, multiplying them pointwise, and then taking the inverse transform.

\begin{problem}
\emph{This Problem is optional.  If the instructor does not require it then students may use the provided \texttt{pulse.wav} file which contains the sound of a balloon pop in a large room.}

Find a large room or area with good acoustics, and record (an approximation to) its impulse response using a balloon pop. To record the sound, you will want to use at least a decent microphone. You may want to record it using the program Audacity \footnote{Audacity is free software and may be downloaded at http://audacity.sourceforge.net} and a laptop. If you use a unidirectional microphone, be sure the microphone is pointing at the balloon when you pop it, so that the direct sound from the pop is picked up. (If you don't, the result will still be okay. It's just that after we do the convolution, it will probably sound somewhat distant, as if we are at the back of the room, where we can't hear the music directly, only through the reverberation of the room.)
If you've chosen a good room, the response should be audible for at least a full second.

Include a plot of both the waveform and spectrum of the impulse response you recorded.
\end{problem}


\begin{problem}\label{convolution_problem}
Download and listen to the file \texttt{chopinw.wav}. You will hear a piano being played in a dead room with little or no acoustics. Using the Convolution Theorem, take the convolution of this signal with the impulse response recorded in the previous problem. Describe the resulting sound.

In doing this problem, keep in mind that the Convolution Theorem requires both signals to have the same length; therefore you will need to pad the smaller of your two signals (namely, the impulse response signal) with zeros at the end in order to make it the same size as the other signal. But the convolution of the Convolution Theorem is a circular convolution, which will mean that the room's response to a sound near the end of the signal will wrap around back to the beginning of the signal in the result. In order to avoid this undesired effect, you will want to also pad the signals with additional zeros (at least as many zeros as one less than the size of the impulse response signal).
\end{problem}

In doing the preceding problem, keep in mind that the Convolution Theorem requires both signals to have the same length; therefore you will need to pad the smaller of your two signals (namely, the impulse response signal) with zeros at the end in order to make it the same size as the other signal. But the convolution of the Convolution Theorem is a circular convolution, which will mean that the room's response to a sound near the end of the signal will wrap around back to the beginning of the signal in the result. In order to avoid this undesired effect, you will want to also pad the signals with additional zeros (at least as many zeros as one less than the size of the impulse response signal).

In some instances, a circular convolution is actually desirable. For instance, an interesting effect is achieved by taking the circular convolution of a long segment of white noise with some other (shorter) sound. We can create white noise using scipy's \texttt{rand} function:
\begin{lstlisting}
#samplerate = 22050
#noise = sp.float32(sp.random.randint(-32767,32767,samplerate*10)) # Create 10 seconds of mono white noise
\end{lstlisting}

TODO float32?

\begin{problem}
Create white noise in this way this and listen to the resulting sound (CAUTION: Turn your volume way down.  It may be very very loud).  This kind of noise is called ``white" because it contains all frequencies (below the Nyquist frequency) with the same strength, or rather, with the same expected strength (since the amplitude of a specific frequency is a matter of chance). In order to see this, plot the spectrum of the noise.
\end{problem}

Now we are going to take the circular convolution of this noise with some other sound. For instance, let's use \texttt{Tada.wav}. The result is in \texttt{tada-conv.wav}. We notice that the original short sound has been sustained to an indefinite length. The result is not a set of static tones, but rather a rich sound which preserves not only the tones, but the texture, of the original sound; you can hear different tones fluctuating randomly in amplitude over time. If you were to play this \texttt{tada-conv.wav} on repeat, you would find that, because we used a circular convolution, the sound loops seamlessly from the end back to the beginning; however, most sound players are not capable of doing this properly, so you will probably hear a break in the sound. To demonstrate the ``seamlessness", we can paste together three copies of the sound consecutively:

\begin{lstlisting}
rate, sig = wavfile.read('tada-conv.wav')
sig = sp.append(sig,sig)
sig = sp.append(sig,sig)
\end{lstlisting}

Listen to the resulting sound, and notice that we are not able to identify where the sound loops back to the beginning, because there is no break or click.

\begin{problem}
Record yourself singing a few notes (or, feel free to produce some other sound another way). Take the circular convolution of white noise with this recording. Now do it again using stereo white noise, which you can create, e.g., by doing \texttt{noise = sp.float16(sp.random.randint(-32767,32767,(samplerate*10,2)))}. It's no problem that your original recording will probably be mono; just make the left and right channels duplicate in the recording (but: be sure to use different left and right channels for the white noise). Can you hear any difference between the mono and stereo versions of the result?
\end{problem}

Feel free to play around with this. The file \texttt{guitar-conv.mp3} is a collage of sounds created using this technique (mostly using guitar samples). You could probably think of other lots of other things you can do with this.



\begin{problem}
Using the algorithm described in the lecture notes, implement your own Inverse Fast Fourier Transform (IFFT) in MATLAB from scratch. You may assume that $N$ is a power of 2. Test it on several random vectors and make sure the results match those of MATLAB's \texttt{ifft}. It should have $O(N\log N)$ running time. Now, by adjusting your IFFT, implement the FFT, and redo Problem \ref{convolution_problem} using the FFT and IFFT which you have implemented. Use \texttt{tic} and \texttt{toc} to measure how long it takes to finish, against how long it took using MATLAB's \texttt{fft} and \texttt{ifft}.
\end{problem}

It is possible to implement your \texttt{fft} and \texttt{ifft} using vector operations, which will speed things up; however, one could hardly expect to beat MATLAB's \texttt{fft} and \texttt{ifft}, which are implemented by calling the extremely well-optimized FFTW library (Fastest Fourier Transform in the West).




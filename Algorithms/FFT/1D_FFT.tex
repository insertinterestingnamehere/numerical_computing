\def\x{{\bf x}}
\def\q{{\bf q}}
\def\p{{\bf p}}
\def\v{{\bf v}}

\lab{Algorithms}{Fast Fourier Transform}{Fast Fourier Transform}
\objective{This lesson explores the fourier transform applied to audio signals.}

\section*{Audio Signals}

In this lab we will working with signals which represent sound. First we should understand how sound is produced and transmitted. As an example, consider a typical electronic speaker.

\begin{center}\includegraphics[width=45mm]{speaker}\end{center}

The essential component is a bowl-shaped membrane, called the \emph{cone} (or \emph{diaphragm}) of the speaker. Behind the cone is attached a lightweight electromagnet. When current flows through the electromagnet in a certain direction, a magnetic field is created which repels the electromagnet (and the attached cone) from a  heavier permanent magnet mounted behind it. The cone is thus thrust forward, compressing the air in front of the speaker, resulting a slight increase in air pressure which is propagated through the room in a wave. When the direction of the current is reversed, the electromagnet is attracted to the permanent magnet. The cone is thus pulled back, expanding the air in front of the speaker, resulting in a slight decrease in air pressure which, again, is propagated through the room in a wave. These pressure waves reach your ears and are perceived as sound. The basic mechanism of a speaker is entirely reversible; for instance, a pair of ordinary (non-amplifying) headphones can also, without any modification, be used as a stereo microphone (Try it!).

Play the audio clip \texttt{pulseramp.wav}. You will hear a series of pulses, or clicks. A click is simply a sudden (small) increase in pressure followed almost immediately by a return to normal pressure. By a speaker, a click is produced by the cone suddenly thrusting forward a small amount and then gradually, silently, returning to its original position. Notice that as the pulses occur closer and closer together, eventually your ears no longer perceive them as individual pulses. Instead they are perceived as a tone which increases in pitch as the pulses increase in frequency.

In python we can read in a \texttt{.wav} by importing wavfile from scipy.io and using wavfile.read() as follows
\begin{lstlisting}
from scipy.io import wavfile
rate, mywave = wavfile.read('pulseramp.wav')
\end{lstlisting}
Here \texttt{data} is the signal and \texttt{rate} is the sampling rate of the audio measured in Hz.  We can check the sampling rate and the size of the signal:
\begin{lstlisting}
rate
Out[2]: 44100

mywave.shape
Out[3]: (300000,)
\end{lstlisting}
We find that \texttt{mywave} is a vector of length $300000$. It has only one column because this is a mono (single-channel) \texttt{.wav} file. If we had used a stereo \texttt{.wav} file, then mywave would have been a matrix with two columns, one for the left channel and one for the right.

The entries of \texttt{mywave} are integers between $-32767$ and $+32767$ which represent the relative air pressure at a particular instant, sampled at regular intervals (these bounds are the range of 16-bit integers).   In our case, the \texttt{.wav} file is sampled at 44.1KHz (the standard sample-rate for high-quality audio), so there are 44100 samples per second. $+32767$ represents the maximum possible increase in air pressure which the speaker can produce (at its current volume level), while $-32767$ represents the maximum possible decrease in air pressure. In the case of our particular \texttt{.wav} file, you can check that most of the entries of \texttt{mywave} are $0$, indicating neutral pressure. Only occasionally you will find an entry which is $+32767$, which is where a pulse occurs.
We can see this most easily if we plot the waveform of our sound (see Figure \ref{pulseramp}):
\begin{lstlisting}
from matplotlib import pyplot as plt
plt.plot(mywave)
plt.show()
\end{lstlisting}
\begin{figure}[ht]\caption{A sequence of pulses of increasing frequency}\label{pulseramp}\centering\includegraphics[width=\textwidth]{pulseramp}\end{figure}

We should keep in mind that the deviations in air pressure that we hear as sound are extremely small relative to atmospheric pressure. For instance, in normal conversation, the deviation in air pressure reaching your ears might be around $.01$ pascals, about one ten-millionth of atmospheric pressure.


\section*{Using the Fourier Transform to Investigate the Spectrum of a Signal}

In many ways the most basic kind of sound is one whose waveform is a sinusoid (i.e., a scaled, shifted version of the sine function). The tone of a sinusoid is, to the ear, more mellow than any other sound of the same pitch. We have seen that, mathematically, sinusoids are important, because Fourier series provide a way of expressing any periodic, piecewise continuous function as a sum of sine and cosine waves of various frequencies and amplitudes, or, more conveniently, using complex exponential functions.

Here's one way of producing a a 1760Hz sine wave (which in musical terms is A6, the A between two and three octaves above middle C):

\begin{lstlisting}
import scipy as sp
from scipy.io import wavfile

samplerate=44100 # 44100 samples per second
freq=1760 # We're going to produce a 1760 Hz sine wave ...
length=2 # ... which will last for 2 seconds.
stepsize=freq*2*sp.pi/samplerate
sig=sp.sin(sp.arange(0,stepsize*length*samplerate,stepsize))
scaled = sp.int16(sig/sp.absolute(sig).max() * 32767)
wavfile.write('sinusound.wav',samplerate, scaled)
\end{lstlisting}

Notice before writing the file we had to scale it to be a 16-bit integer between -32767 and +32767 because wavfile.write expects the data to be in this form.  Now play this sound.

\begin{problem}
Produce a 60Hz sine wave 1 second in length. Then plot the signal. Play the sound. Is it audible?
\end{problem}


For a discrete signal, we are given finitely many data points (assumed to be evenly spaced on the $x$-axis); we can think of such a signal as a function $f$ whose domain consists of the integers $\{0,1,\dots,N-1\}$, where $N$ is the number of data points. For convenience, we will often extend $f$ to be a periodic function with period $N$, defined on all integers, by defining $f(a+bN)=f(a)$ for all integers $a$ and $b$. How might we find something analagous to a Fourier series for such a function? Well, it turns out that the functions, $e^{\frac{2\pi ikx}N}$, which are periodic with period $N$, form an orthonormal set with respect to the inner product
$$\langle f,g\rangle = \frac1N\sum_{k=0}^{N-1}\overline{f(k)}g(k)$$
In particular, these functions are linearly independent and hence form a basis for the $N$-dimensional (complex) vector space of complex-valued functions with domain $\{0,\dots,N-1\}$. The vector of coordinates of a function $f$ with respect to this basis constitutes what is called the \emph{Discrete Fourier Transform} (or \emph{DFT}) of the function $f$. In other words, any function $f$ whose domain consists of the integers $\{0,1,\dots,N-1\}$ may be expressed
$$f(x)=\sum_{k=0}^{N-1}c_ke^{\frac{2\pi ikx}N}$$
and we call the vector of coefficients $(c_0,c_1,\dots,c_{N-1})$ the DFT of $f$. Explicitly, these coefficients may be computed by taking the inner product of each basis function with $f$:
$$c_k=\langle e^{\frac{2\pi ikx}N}, f(x)\rangle=\frac1N\sum_{x=0}^{N-1}e^{\frac{-2\pi ikx}N}f(x)$$

In scipy, there is a module \texttt{scipy.fft} that contains functions for computing the DFT of a signal. (\texttt{fft} stands for Fast Fourier Transform.) For example, if we create a sine wave signal \texttt{sig} as above and then type,
\begin{lstlisting}
from scipy import fft
fsig = fft(sig)
\end{lstlisting}
then \texttt{fsig} is an array of complex numbers representing the DFT of this signal. We will plot the absolute values of the entries of \texttt{fsig} (see Figure \ref{sinespec}).

Notice that there are two spikes, one on the left and one on the right. Why are there two spikes when there is only one tone? The answer is that for a real-valued signal, the right half of the DFT is always a mirror-image duplicate of the left half -- or, more precisely, each entry in the right half of the DFT is the complex conjugate of the corresponding entry in the left half. Thus the right half of the DFT gives us no new information, and we may as well ignore it.

In saying that the right half of the DFT is a complex-conjugate mirror-image of the left half, to be perfectly correct we actually need to ignore the very first entry of the DFT.


\begin{problem}
Implement a function to compute the DFT of a vector using the naive algorithm, directly from the definition of the DFT. Test it on several random vectors and make sure the results match those of scipy's \texttt{fft}.  Note that the scaling scipy uses does not divide by $N$, so you will need to account for that in comparing your results.
\end{problem}

The naive way of computing the DFT has $O(N^2)$ runtime.  There is a very important algorithm (and many variations of it) called the Fast Fourier Transform (FFT) which computes the DFT in $O(N \log N)$ runtime.  Essentially the FFT algorithm accomplishes this speedup by breaking the recursively breaking up the original array into smaller pieces, finding the Fourier Transform of those smaller pieces, and carefully recombining to get the Fourier Transform of the original array.  While scipy uses the FFT algorithm, it's implementation at the time of this writing is somewhat slow.

There is a library written in $C$ for computing the FFT called FFTW (Fastest Fourier Transform in the West) that is substantially quicker.  There is a python package available for download called ``anfft'' that wraps fftw in python.  Download it from https://code.google.com/p/anfft/ and install it for use in the remaining problems.

\begin{figure}[ht]\caption{DFT of a sine wave}\label{sinespec}\centering\includegraphics[width=\textwidth]{sinespec}\end{figure}

Now let's look at a more complicated signal. Play the file \texttt{tada.wav}.  If you read \texttt{tada.wav} into python you will find it has two columns.  That is because it is a stereo recording meaning the two columns correspond to two audio signals (left and right).  When using anfft, it will take the fourier transform along the last axis.  Because of this we will take the transpose before computing the FFT and again after to return to the original shape.  Also note that anfft expects signals to have scipy's float16 or float32 as the datatype.  Consequently after reading in the signal we cast it as a float32.

Now we plot the DFT of \texttt{tada.wav} (see Figure \ref{tadaspec}):
\begin{lstlisting}
import scipy as sp
from scipy.io import wavfile
import anfft

rate, sig = wavfile.read('tada.wav')
sig = sp.float32(sig)
fsig = anfft.fft(sig.T).T
plt.figure()
plt.plot(sp.absolute(fsig))
plt.show()
\end{lstlisting}


\begin{figure}[ht]\caption{Absolute value of the DFT of \texttt{Tada.wav}}\label{tadaspec}\centering\includegraphics[width=\textwidth]{tadaspec}\end{figure}
Again, the right half of the plot is a mirror-image duplicate of the left half. We might as well exclude it by only plotting the left half. The absolute value of the left half of  the DFT is called the \emph{spectrum} of the signal (see Figure \ref{tadaspec2}):
\begin{lstlisting}
f = sp.absolute(fsig)
plt.plot(f[0:f.shape[0]/2,:])
plt.show()
\end{lstlisting}
\begin{figure}[ht]\caption{Spectrum of \texttt{Tada.wav} (Absolute value of left half of DFT)}\label{tadaspec2}\centering\includegraphics[width=\textwidth]{tadaspec2}\end{figure}
Notice that the number of spikes in the plot is much greater than the number of musical notes in the sound. This is because when a one note is played on a musical instrument, more than one frequency is produced: the lowest frequency is called the \emph{fundamental}; the others, which usually occur as integer multiples of the fundamental frequency, are called \emph{overtones}. In instruments with a mellow tone (such as a flute or vibraphone) the overtones are relatively soft, while in instruments with a brighter tone (such as a trumpet) the overtones are quite prominent. In the DFT of \texttt{Tada.wav}, you will notice spikes at about 2030, 4060, 6090, 8120, 10160, 12190, 14220, 16250, 18280, and 20320 units. These are approximately integer multiples of 2030 and represent overtones of a single musical note whose fundamental frequency is about 2030 units.

Now what are these units and how can we express them in Hz? Well, a frequency of 2030 units in the DFT means that the tone undergoes 2030 cycles through the extent of the entire recording. We use check the shape of \texttt{sig} to find the total number of samples in the recording:
\begin{verbatim}
sig.shape
Out[27]: (42752, 2)
\end{verbatim}
So the entire recording is 42752 samples long (in each channel -- this is a stereo recording.) The sample rate is 22050 samples per second. (You could find this out, for instance, by right clicking on the \texttt{.wav}, selecting \emph{Properties} and looking under the \emph{Summary} tab).  Thus, 2030 cycles through the entire recording is equivalent to
$$\frac{2030\text{ cycles}}{42752\text{ samples}} \times \frac{22050\text{ samples}}{\text{sec}} \approx 1047 \text{ Hz}.$$
So this tone is at 1047 Hz, which in musical terms is a (concert) C6, the C two octaves above middle C. There are also other, lower musical notes in the recording which each have their own sequence of overtones.

\begin{problem}
Listen to \texttt{pianoclip.wav}. Identify the audible frequency of greatest amplitude (Ignore the 0 frequency as explained in the following paragraph.).  What musical note does the fundamental frequency correspond to? (For this, you may wish to use the table on http://en.wikipedia.org/wiki/Piano\_key\_frequencies)
\end{problem}

You will notice that in this case the largest spike actually occurs in the very first entry of the DFT. This corresponds to a frequency of 0Hz. A cosine wave of 0Hz is nothing other than a constant function. The presence of this 0Hz frequency, if taken literally, would mean that there was a (slight) sustained increase above the normal atmospheric pressure throughout the recording. However, a speaker or musical instrument, because of its bounded ranges of motion, is incapable of producing a sustained increase (or decrease) in pressure; and a microphone, for the same reason, is incapable of detecting such a sustained change in pressure. An ordinary speaker cannot produce frequencies below about 15 to 20Hz, and even if it could, we would not be able to hear them. The 0Hz frequency which appears here is simply an artifact of an imperfect recording device. It indicates that some direct current ``leaked" through the device.



\section*{Down-sampling a signal}

Listen to \texttt{saw.wav}. You will hear a high-pitched buzzing sound \footnote{The technical term for it is a sawtooth wave; for the curious, see http://en.wikipedia.org/wiki/Sawtooth\_wave}. The sound is recorded at a sample rate of 44.1KHz. Now let's say we wanted to convert it to a 22.05KHz sample rate. How might we do that? Since we're cutting the sample rate in half, we might think that we could accomplish this simply by discarding every other sample, i.e., keep the even-index samples and discard the odd-index samples. Let's try that and see how it works:
\begin{lstlisting}
rate, in_sig = wavfile.read('saw.wav')
out_sig = in_sig[sp.arange(0,rate,2)]
wavfile.write('down_saw.wav',rate/2,out_sig)
\end{lstlisting}
What does it sound like? Well, it sounds much like the original except that there are some extra tones which weren't there to begin with; these extra tones, in this case, are not in a nice harmonic relationship with the original tone and clash rather badly. One of the extra tones is at a rather low frequency of about 99Hz compared to 1568Hz of the fundamental frequency in the original tone. Where are these mysterious ``extra" tones coming from?

To explain this, we need to understand some important facts about sampling. First of all, a basic fact is that the highest possible frequency that a sampled signal can represent is half the sampling rate. Such a frequency is represented by alternating between +32767 and -32767 at each sample. This frequency is known as the \emph{Nyquist frequency}. If, for instance, the sampling rate is 44.1KHz, then the Nyquist frequency will be 22.05KHz. This is slightly beyond the upper limit of human hearing; thus, for audio, generally no improvement in quality results from using a higher sampling rate than 44.1KHz, since at 44.1KHz the entire audible spectrum can already be represented.

Now let's consider what would happen if we tried to represent a frequency beyond the Nyquist frequency. We create a sinusoid of frequency $f$ Hz by setting
$$s_k = \cos\left(\frac{2\pi kf}r\right)$$
where $s_k$ is the value of the $k$th sample and $r$ is the sample rate in Hz. Suppose we take the frequency to be $f=\frac{r}2+a$, where $a$ is some positive number, so that $f$ is beyond the Nyquist frequency. Then we have
\begin{align*}
s_k &= \cos\left(\frac{2\pi kf}r\right) \\
&= \cos\left(\frac{2\pi k(\frac{r}2+a)}r\right)\\
&= \cos\left(\pi k+\frac{2\pi ka}r\right)\\
&= \cos\left(-\left(\pi k+\frac{2\pi ka}r\right)\right)\\
&= \cos\left(-\left(\pi k+\frac{2\pi ka}r\right)+2\pi k \right)\\
&= \cos\left(\pi k-\frac{2\pi ka}r\right)\\
&= \cos\left(\frac{2\pi k(\frac{r}2-a)}r\right).
\end{align*}
But this is a sinusoid of frequency $\frac{r}2-a$, which is a frequency below the Nyquist frequency. Thus a sinusoid above the Nyquist frequency, when sampled, is indistinguishable from a corresponding sinusoid below the Nyquist frequency. An electronic speaker, given the sampled signal, will actually produce the sinusoid which lies below the Nyquist frequency, not the one above. Thus,
\emph{A frequency above the Nyquist frequency will, when sampled, have its frequency reflected across the Nyquist frequency.}
This phenomenon is known as \emph{aliasing}.

What does this have to do with our problem? Our tone is at 1568Hz, which is well below the Nyquist frequency of 22050Hz. However, there are prominent overtones extending all the way up to 21951Hz (see Figure \ref{sawspec}). Since we are downsampling to 22050Hz, the new Nyquist frequency will be 11025Hz. The overtones in our signal which lie between 11025Hz and 22050Hz will then be reflected across the new Nyquist frequency in our downsampled sound. For example, the overtone at 21951Hz will be reflected to $11025-(21951-11025)=99$ Hz, the lowest ``extra" tone that we observed. See Figure \ref{sawspecdown}.

\begin{figure}[ht]\caption{Spectrum of \texttt{saw.wav}}\label{sawspec}\centering\includegraphics[width=\textwidth]{sawspec}\end{figure}

\begin{figure}[ht]\caption{Spectrum of \texttt{saw.wav} naively downsampled to 22050Hz}\label{sawspecdown}\centering\includegraphics[width=\textwidth]{sawspecdown}\end{figure}

So, how do we downsample our signal in a way that avoids this unwanted aliasing effect? What we do is take the DFT of our signal, cut out the upper half of the spectrum (which can no longer be represented, after we downsample), then take the inverse DFT of the result. It's as simple as that.

Well -- \emph{almost} that simple. Actually, when we say we're cutting out a set of frequencies from the spectrum, we mean we're removing those frequencies from the left half of the DFT and also the corresponding frequencies from the right half of the DFT. So, we're actually removing a big chunk out of the \emph{middle} of the DFT and then pasting the two ends back together. We can do this as follows where in\_sig is the signal from \texttt{saw.wav}:
\begin{lstlisting}
old_rate = 44100
new_rate = 22050
in_sig = sp.float32(in_sig)
fin = anfft.fft(in_sig)
nsiz = sp.floor(in_sig.size*new_rate/old_rate)
nsizh = sp.floor(nsiz/2)
fout = sp.zeros(nsiz)
fout = fout + 0j
fout[0:nsizh] = fin[0:nsizh]
fout[nsiz-nsizh+1:] = sp.conj(sp.flipud(fout[1:nsizh]))
out = anfft.ifft(fout)
\end{lstlisting}
There are just a couple minor problems we need to deal with before we're finished. First, because of roundoff error, you will find that some of the entries of the \texttt{out} have (slightly) nonzero imaginary components.  So we should first make sure the signal is purely real,
\begin{lstlisting}
out = sp.real(out) % Take the real component of the signal
\end{lstlisting}
Second, as always, we need to rescale and cast as a 16-bit integer:
\begin{lstlisting}
out = sp.int16(out/sp.absolute(out).max() * 32767)
\end{lstlisting}


\begin{figure}[ht]\caption{Spectrum of \texttt{saw.wav} correctly downsampled to 22050Hz}\label{sawspecdown2}\centering\includegraphics[width=\textwidth]{sawdownspec}\end{figure}

\begin{problem}
Convert the same sound, \texttt{saw.wav}, to a 11025Hz sampling rate. Do you notice a difference in the quality of the sound? If so, can you explain why?
\end{problem}

\begin{problem}
Can the technique above be used to change the sampling rate by something other than an integer factor, e.g., from 44100Hz to 36000Hz? Try it.
\end{problem}

\begin{problem}
Suppose we would like to upsample \texttt{Tada.wav} from a sampling rate of 22050Hz to 44100Hz. \begin{enumerate}
\item[(a)]
The naive way to do this would be to use linear interpolation on the original signal, i.e., the original signal would form the odd-index samples of the new signal, while the even-index samples of the new signal would be formed by taking the average of each two consecutive samples in the original signal. Write code to carry out the upsampling in this way; plot the spectrum of the result. What do you observe?
\item[(b)]Now write code to upsample the signal using the DFT, using a technique similar to what we did above for downsampling. Again, plot the spectrum of the result. Can you hear any difference between this and the result of (a)?
\end{enumerate}
\end{problem}

